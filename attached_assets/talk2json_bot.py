# -*- coding: utf-8 -*-
"""Talk2JSON_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WbbIC3PFKScAw9Qn2oXppwt6_Ok2Cj4Q
"""

import json
import os
import re
from typing import List, Dict, Any, Tuple
from rank_bm25 import BM25Okapi
import ipywidgets as widgets
from IPython.display import display
import requests
from google.colab import drive, userdata
import numpy as np
from collections import defaultdict
import pickle
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
GOOGLE_DRIVE_PATH = "/content/drive/MyDrive/txt2json_data/knowledge_base.json"
API_URL = "https://api.vsegpt.ru/v1/chat/completions"
API_KEY = userdata.get('API_KEY')
CONTEXT_SUM = 4000
MAX_ANSWER_LENGTH = 4000
MAX_HISTORY_LENGTH = 100
TEMPERATURE = 0.4
BM25_CACHE_PATH = "/content/drive/MyDrive/txt2json_data/bm25_cache.pkl"

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive
drive.mount('/content/drive')

# –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
RUSSIAN_STOPWORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ',
    '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞',
    '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç',
    '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏',
    '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å',
    '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏',
    '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞',
    '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç',
    '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º',
    '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å',
    '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞',
    '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å',
    '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º',
    '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è',
    '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
}

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')

    def preprocess(self, text: str) -> List[str]:
        text = self.regex.sub(' ', text.lower())
        return text.split()  # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞


class BM25SearchEngine:
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.bm25 = None
        self.chunks_info = []
        self.doc_index = defaultdict(list)
        self.is_index_loaded = False

    def build_index(self, knowledge_base: List[Dict]) -> None:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        print("–ù–∞—á–∞–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞...")
        corpus = []
        self.chunks_info = []

        for doc_idx, doc in enumerate(knowledge_base):
            for chunk in doc.get('chunks', []):
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                text_parts = [
                    str(chunk.get('chunk_summary', '')),
                    ' '.join(map(str, chunk.get('chunk_keywords', []))),
                    str(chunk.get('chunk_text', ''))
                ]
                text_for_index = ' '.join(text_parts)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                self.chunks_info.append({
                    'doc_id': doc.get('doc_id', f"doc_{doc_idx}"),
                    'doc_name': doc.get('doc_name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
                    'chunk_summary': chunk.get('chunk_summary', ''),
                    'chunk_text': chunk.get('chunk_text', ''),
                    'chunk_keywords': chunk.get('chunk_keywords', [])
                })

                corpus.append(text_for_index)

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(corpus)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        tokenized_corpus = [self.preprocessor.preprocess(doc) for doc in corpus]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.is_index_loaded = True
        print("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def load_from_cache(self, cache_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –∫—ç—à–∞"""
        try:
            if not os.path.exists(cache_path):
                print("–§–∞–π–ª –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return False

            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
                if len(data) != 3:
                    print("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫—ç—à–∞")
                    return False

                self.bm25, self.chunks_info, self.doc_index = data
                self.is_index_loaded = True
                print("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞")
                return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {str(e)}")
            return False

    def save_to_cache(self, cache_path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –∫—ç—à"""
        try:
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            with open(cache_path, 'wb') as f:
                pickle.dump((self.bm25, self.chunks_info, self.doc_index), f)
            print(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à: {cache_path}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {str(e)}")

    def search(self, query: str, top_n: int = 5, score_threshold: float = 0.1) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"""
        if not self.is_index_loaded:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        tokens = self.preprocessor.preprocess(query)
        if not tokens:
            return []

        scores = self.bm25.get_scores(tokens)
        if len(scores) == 0:
            return []

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        best_indices = np.argsort(scores)[-top_n:][::-1]

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for idx in best_indices:
            if scores[idx] > score_threshold:
                result = {**self.chunks_info[idx], 'score': float(scores[idx])}
                results.append(result)

        return results

def load_knowledge_base(file_path: str) -> List[Dict]:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('documents', [])

def build_llm_context(query: str, chunks: List[Dict], history: str = "") -> str:
    context_parts = [
        f"–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}",
        "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:"
    ]

    for chunk in sorted(chunks, key=lambda x: x.get('score', 0), reverse=True)[:5]:
        context_parts.extend([
            f"\n–î–æ–∫—É–º–µ–Ω—Ç: {chunk.get('doc_name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}",
            f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(chunk.get('chunk_keywords', []))}",
            f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {chunk.get('chunk_text', '')[:1000]}"
        ])

    if history:
        context_parts.append(f"\n–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{history}")

    return '\n'.join(context_parts)[:CONTEXT_SUM]

class LLMClient:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def query(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        payload = {
            "model": "anthropic/claude-3-haiku",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }

        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=30
            )

            if response.status_code != 200:
                raise ValueError(f"API error: {response.status_code}")

            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            raise ConnectionError(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")

class ChatInterface:
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.search_engine = BM25SearchEngine(self.preprocessor)
        self.llm_client = LLMClient(API_URL, API_KEY)
        self.history = []

    def initialize(self):
        print("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        start_time = time.time()

        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
        if not self.search_engine.load_from_cache(BM25_CACHE_PATH):
            print("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            try:
                knowledge_base = load_knowledge_base(GOOGLE_DRIVE_PATH)
                if not knowledge_base:
                    raise ValueError("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")

                self.search_engine.build_index(knowledge_base)
                self.search_engine.save_to_cache(BM25_CACHE_PATH)
            except Exception as e:
                raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")

        print(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time()-start_time:.2f} —Å–µ–∫")
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(self.search_engine.chunks_info)}")

    def process_query(self, query: str) -> str:
        print("üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...")
        chunks = self.search_engine.search(query)

        if not chunks:
            return "‚ùå –ü–æ –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        context = build_llm_context(query, chunks, self.get_history())
        print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")

        messages = [
            {"role": "system", "content": "–¢—ã - AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç—ã. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç–µ–π –∏ –ø—É–Ω–∫—Ç–æ–≤."},
            {"role": "user", "content": context}
        ]

        answer = self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)
        self.add_to_history(query, answer)
        return answer

    def add_to_history(self, query: str, answer: str) -> None:
        self.history.append((query, answer))
        if len(self.history) > MAX_HISTORY_LENGTH:
            self.history.pop(0)

    def get_history(self) -> str:
        return "\n".join(f"–í: {q}\n–û: {a}" for q, a in self.history)

def main_interface():
    try:
        chat = ChatInterface()
        chat.initialize()

        query_input = widgets.Textarea(
            value='',
            placeholder='–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å...',
            description='–ó–∞–ø—Ä–æ—Å:',
            layout={'width': '80%', 'height': '100px'}
        )

        output = widgets.Output()
        submit_button = widgets.Button(description="–û—Ç–ø—Ä–∞–≤–∏—Ç—å")

        def on_submit(b):
            with output:
                output.clear_output()
                query = query_input.value.strip()
                if not query:
                    print("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞")
                    return

                try:
                    answer = chat.process_query(query)
                    print(f"\nüí° –û—Ç–≤–µ—Ç:\n{answer}")
                except Exception as e:
                    print(f"üö® –û—à–∏–±–∫–∞: {str(e)}")

        submit_button.on_click(on_submit)
        display(widgets.VBox([query_input, submit_button, output]))

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")

if __name__ == "__main__":
    main_interface()