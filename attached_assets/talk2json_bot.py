# -*- coding: utf-8 -*-
"""Talk2JSON_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WbbIC3PFKScAw9Qn2oXppwt6_Ok2Cj4Q
"""

import json
import os
import re
from typing import List, Dict, Any, Tuple
from rank_bm25 import BM25Okapi
import ipywidgets as widgets
from IPython.display import display
import requests
from google.colab import drive, userdata
import numpy as np
from collections import defaultdict
import pickle
import time

# Настройки
GOOGLE_DRIVE_PATH = "/content/drive/MyDrive/txt2json_data/knowledge_base.json"
API_URL = "https://api.vsegpt.ru/v1/chat/completions"
API_KEY = userdata.get('API_KEY')
CONTEXT_SUM = 4000
MAX_ANSWER_LENGTH = 4000
MAX_HISTORY_LENGTH = 100
TEMPERATURE = 0.4
BM25_CACHE_PATH = "/content/drive/MyDrive/txt2json_data/bm25_cache.pkl"

# Подключение Google Drive
drive.mount('/content/drive')

# Собственные стоп-слова для русского языка
RUSSIAN_STOPWORDS = {
    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то',
    'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за',
    'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет',
    'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если',
    'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять',
    'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они',
    'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была',
    'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет',
    'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем',
    'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас',
    'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два',
    'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас',
    'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем',
    'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя',
    'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'
}

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')

    def preprocess(self, text: str) -> List[str]:
        text = self.regex.sub(' ', text.lower())
        return text.split()  # Простая токенизация без стоп-слов и стемминга


class BM25SearchEngine:
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.bm25 = None
        self.chunks_info = []
        self.doc_index = defaultdict(list)
        self.is_index_loaded = False

    def build_index(self, knowledge_base: List[Dict]) -> None:
        """Построение поискового индекса"""
        print("Начало построения индекса...")
        corpus = []
        self.chunks_info = []

        for doc_idx, doc in enumerate(knowledge_base):
            for chunk in doc.get('chunks', []):
                # Подготовка текста для индексации
                text_parts = [
                    str(chunk.get('chunk_summary', '')),
                    ' '.join(map(str, chunk.get('chunk_keywords', []))),
                    str(chunk.get('chunk_text', ''))
                ]
                text_for_index = ' '.join(text_parts)

                # Сохраняем метаданные
                self.chunks_info.append({
                    'doc_id': doc.get('doc_id', f"doc_{doc_idx}"),
                    'doc_name': doc.get('doc_name', 'Без названия'),
                    'chunk_summary': chunk.get('chunk_summary', ''),
                    'chunk_text': chunk.get('chunk_text', ''),
                    'chunk_keywords': chunk.get('chunk_keywords', [])
                })

                corpus.append(text_for_index)

        # Токенизация и создание индекса
        print(f"Обработка {len(corpus)} документов...")
        tokenized_corpus = [self.preprocessor.preprocess(doc) for doc in corpus]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.is_index_loaded = True
        print("Построение индекса завершено")

    def load_from_cache(self, cache_path: str) -> bool:
        """Загрузка индекса из кэша"""
        try:
            if not os.path.exists(cache_path):
                print("Файл кэша не найден")
                return False

            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
                if len(data) != 3:
                    print("Неверный формат кэша")
                    return False

                self.bm25, self.chunks_info, self.doc_index = data
                self.is_index_loaded = True
                print("Индекс успешно загружен из кэша")
                return True

        except Exception as e:
            print(f"Ошибка загрузки кэша: {str(e)}")
            return False

    def save_to_cache(self, cache_path: str) -> None:
        """Сохранение индекса в кэш"""
        try:
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            with open(cache_path, 'wb') as f:
                pickle.dump((self.bm25, self.chunks_info, self.doc_index), f)
            print(f"Индекс сохранен в кэш: {cache_path}")
        except Exception as e:
            print(f"Ошибка сохранения кэша: {str(e)}")

    def search(self, query: str, top_n: int = 5, score_threshold: float = 0.1) -> List[Dict]:
        """Поиск релевантных фрагментов"""
        if not self.is_index_loaded:
            raise ValueError("Индекс не загружен")

        tokens = self.preprocessor.preprocess(query)
        if not tokens:
            return []

        scores = self.bm25.get_scores(tokens)
        if len(scores) == 0:
            return []

        # Получаем топ-N результатов
        best_indices = np.argsort(scores)[-top_n:][::-1]

        # Фильтрация и форматирование результатов
        results = []
        for idx in best_indices:
            if scores[idx] > score_threshold:
                result = {**self.chunks_info[idx], 'score': float(scores[idx])}
                results.append(result)

        return results

def load_knowledge_base(file_path: str) -> List[Dict]:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('documents', [])

def build_llm_context(query: str, chunks: List[Dict], history: str = "") -> str:
    context_parts = [
        f"Запрос пользователя: {query}",
        "Релевантные фрагменты из документов:"
    ]

    for chunk in sorted(chunks, key=lambda x: x.get('score', 0), reverse=True)[:5]:
        context_parts.extend([
            f"\nДокумент: {chunk.get('doc_name', 'Без названия')}",
            f"Ключевые слова: {', '.join(chunk.get('chunk_keywords', []))}",
            f"Содержание: {chunk.get('chunk_text', '')[:1000]}"
        ])

    if history:
        context_parts.append(f"\nИстория диалога:\n{history}")

    return '\n'.join(context_parts)[:CONTEXT_SUM]

class LLMClient:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

    def query(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        payload = {
            "model": "anthropic/claude-3-haiku",
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }

        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=30
            )

            if response.status_code != 200:
                raise ValueError(f"API error: {response.status_code}")

            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            raise ConnectionError(f"Ошибка запроса: {str(e)}")

class ChatInterface:
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.search_engine = BM25SearchEngine(self.preprocessor)
        self.llm_client = LLMClient(API_URL, API_KEY)
        self.history = []

    def initialize(self):
        print("⏳ Инициализация системы...")
        start_time = time.time()

        # Пытаемся загрузить из кэша
        if not self.search_engine.load_from_cache(BM25_CACHE_PATH):
            print("Создание нового индекса...")
            try:
                knowledge_base = load_knowledge_base(GOOGLE_DRIVE_PATH)
                if not knowledge_base:
                    raise ValueError("База знаний пуста")

                self.search_engine.build_index(knowledge_base)
                self.search_engine.save_to_cache(BM25_CACHE_PATH)
            except Exception as e:
                raise RuntimeError(f"Ошибка построения индекса: {str(e)}")

        print(f"✅ Инициализация завершена за {time.time()-start_time:.2f} сек")
        print(f"Загружено фрагментов: {len(self.search_engine.chunks_info)}")

    def process_query(self, query: str) -> str:
        print("🔍 Поиск релевантной информации...")
        chunks = self.search_engine.search(query)

        if not chunks:
            return "❌ По запросу ничего не найдено"

        context = build_llm_context(query, chunks, self.get_history())
        print("🤖 Генерация ответа...")

        messages = [
            {"role": "system", "content": "Ты - AI ассистент, анализирующий документы. Ссылайся на номер статей и пунктов."},
            {"role": "user", "content": context}
        ]

        answer = self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)
        self.add_to_history(query, answer)
        return answer

    def add_to_history(self, query: str, answer: str) -> None:
        self.history.append((query, answer))
        if len(self.history) > MAX_HISTORY_LENGTH:
            self.history.pop(0)

    def get_history(self) -> str:
        return "\n".join(f"В: {q}\nО: {a}" for q, a in self.history)

def main_interface():
    try:
        chat = ChatInterface()
        chat.initialize()

        query_input = widgets.Textarea(
            value='',
            placeholder='Введите запрос...',
            description='Запрос:',
            layout={'width': '80%', 'height': '100px'}
        )

        output = widgets.Output()
        submit_button = widgets.Button(description="Отправить")

        def on_submit(b):
            with output:
                output.clear_output()
                query = query_input.value.strip()
                if not query:
                    print("Введите текст запроса")
                    return

                try:
                    answer = chat.process_query(query)
                    print(f"\n💡 Ответ:\n{answer}")
                except Exception as e:
                    print(f"🚨 Ошибка: {str(e)}")

        submit_button.on_click(on_submit)
        display(widgets.VBox([query_input, submit_button, output]))

    except Exception as e:
        print(f"Ошибка инициализации: {str(e)}")

if __name__ == "__main__":
    main_interface()