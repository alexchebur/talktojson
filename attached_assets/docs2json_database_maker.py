# -*- coding: utf-8 -*-
"""Docs2JSON_database_maker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vInIIz4sAYD6sRs-gV7c7LuSzGuMIS2d
"""

import os
import json
import uuid
import re
from IPython.display import display, clear_output
import ipywidgets as widgets
from PyPDF2 import PdfReader
from google.colab import drive
from google.colab import files
import requests
drive.mount('/content/drive')
from google.colab import userdata


# Проверка доступности docx
try:
    from docx import Document
    HAS_DOCX = True
except ImportError:
    print("Библиотека python-docx не установлена. DOCX файлы не будут поддерживаться.")
    HAS_DOCX = False

# Конфигурация
LLM = "anthropic/claude-3-haiku"
URL_API = "https://api.vsegpt.ru/v1/chat/completions"
API_KEY = userdata.get('API_KEY')
KNOWLEDGE_BASE_PATH = "/content/drive/My Drive/txt2json_data/knowledge_base.json"
CHUNK_SIZE = 12000  # ~3000 токенов (4 символа = 1 токен)
SUMMARY_CONTEXT_SIZE = 3

class DocumentProcessor:
    def __init__(self):
        self.knowledge_base = self.load_knowledge_base()
        self.chunk_counter = 1
        self.setup_ui()
        self.file_paths = []

    def setup_ui(self):
        """Настройка пользовательского интерфейса"""
        # Виджет загрузки файлов
        self.file_upload = widgets.FileUpload(
            description='Выберите файлы',
            multiple=True,
            accept=('.pdf,.txt,.docx' if HAS_DOCX else '.pdf,.txt')
        )

        # Кнопка обработки
        self.process_btn = widgets.Button(
            description='Обработать файлы',
            disabled=False,
            button_style='success',
            tooltip='Нажмите для обработки файлов'
        )
        self.process_btn.on_click(self.on_process_click)

        # Прогресс-бар
        self.progress = widgets.IntProgress(
            value=0,
            min=0,
            max=100,
            description='Прогресс:',
            bar_style='info'
        )

        # Вывод информации
        self.output = widgets.Output()

        # Отображение всех виджетов
        display(widgets.VBox([
            widgets.HTML("<h2>Обработка юридических документов</h2>"),
            self.file_upload,
            self.process_btn,
            self.progress,
            self.output
        ]))

    def on_process_click(self, b):
        """Обработчик нажатия кнопки"""
        with self.output:
            clear_output(wait=True)
            if not self.file_upload.value:
                print("Ошибка: Файлы не выбраны!")
                return

            # Сохраняем загруженные файлы
            self.save_uploaded_files()

            if self.file_paths:
                self.process_files(self.file_paths)
                print("\nОбработка завершена!")
                self.clean_temp_files()

    def save_uploaded_files(self):
        """Сохранение загруженных файлов во временную папку"""
        if not os.path.exists('temp_files'):
            os.makedirs('temp_files')

        self.file_paths = []
        for name, content in self.file_upload.value.items():
            file_path = os.path.join('temp_files', name)
            with open(file_path, 'wb') as f:
                f.write(content['content'])
            self.file_paths.append(file_path)
            print(f"Файл сохранен: {name}")

    def clean_temp_files(self):
        """Очистка временных файлов"""
        for file_path in self.file_paths:
            try:
                os.remove(file_path)
            except:
                pass
        self.file_paths = []

    def read_file(self, file_path):
        """Чтение содержимого файла в зависимости от формата"""
        try:
            if file_path.endswith('.pdf'):
                reader = PdfReader(file_path)
                return "\n".join([page.extract_text() for page in reader.pages])
            elif file_path.endswith('.docx') and HAS_DOCX:
                doc = Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs])
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            print(f"Ошибка чтения файла {file_path}: {str(e)}")
            return ""

    def split_text(self, text):
        """Разделение текста на чанки"""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)
            if current_length + sentence_length > CHUNK_SIZE and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0

            current_chunk.append(sentence)
            current_length += sentence_length

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def process_files(self, file_paths):
        """Обработка списка файлов"""
        for path in file_paths:
            with self.output:
                print(f"\nОбработка файла: {os.path.basename(path)}")

            text = self.read_file(path)
            if not text:
                continue

            chunks = self.split_text(text)
            self.process_document(chunks, path)

        self.save_knowledge_base()

    def process_document(self, chunks, file_path):
        """Обработка одного документа с сохранением исходного текста чанков"""
        doc_id = str(uuid.uuid4())
        doc_data = {
            "doc_id": doc_id,
            "source_file": os.path.basename(file_path),
            "chunks": [],
            "doc_summary": "",
            "doc_keywords": [],
            "doc_qa": []
        }
        prev_summaries = []

        self.progress.max = len(chunks)
        self.progress.value = 0

        for i, chunk_text in enumerate(chunks):  # Переименовали chunk в chunk_text для ясности
            try:
                self.progress.value = i + 1

                if i == 0:
                    response = self.process_first_chunk(chunk_text)  # Передаём полный текст чанка
                else:
                    context = prev_summaries[-SUMMARY_CONTEXT_SIZE:]
                    response = self.process_chunk(chunk_text, context, i == len(chunks)-1)

                chunk_data = self.parse_llm_response(response, i)
                chunk_data['chunk_text'] = chunk_text  # Сохраняем исходный текст чанка
                prev_summaries.append(chunk_data['chunk_summary'])

                if i == 0:
                    doc_data.update({
                        "doc_name": chunk_data.get('doc_name', 'Неизвестно'),
                        "doc_date": chunk_data.get('doc_date', 'Не указана'),
                        "doc_type": chunk_data.get('doc_type', 'Неизвестен')
                    })

                chunk_entry = {
                    "chunk_id": f"{doc_id}-{self.chunk_counter}",
                    "chunk_summary": chunk_data['chunk_summary'],
                    "chunk_text": chunk_data['chunk_text'],  # Теперь содержит полный текст
                    "chunk_keywords": chunk_data.get('chunk_keywords', []),
                    "qa_pairs": chunk_data.get('qa_pairs', [])
                }

                doc_data['chunks'].append(chunk_entry)
                doc_data['doc_qa'].extend(chunk_data.get('qa_pairs', []))
                self.chunk_counter += 1

            except Exception as e:
                with self.output:
                    print(f"Ошибка обработки чанка {i+1}: {str(e)}")

        # Генерация общего саммари
        doc_summary = self.generate_doc_summary(prev_summaries)
        doc_data['doc_summary'] = doc_summary
        doc_data['doc_keywords'] = self.aggregate_keywords(doc_data['chunks'])

        if 'documents' not in self.knowledge_base:
            self.knowledge_base['documents'] = []
        self.knowledge_base['documents'].append(doc_data)

        with self.output:
            print(f"\nДокумент '{doc_data['doc_name']}' успешно обработан")
            print(f"Тип: {doc_data['doc_type']}")
            print(f"Дата: {doc_data['doc_date']}")
            print(f"Всего QA пар: {len(doc_data['doc_qa'])}")

    def load_knowledge_base(self):
        """Загрузка базы знаний из файла"""
        if os.path.exists(KNOWLEDGE_BASE_PATH):
            try:
                with open(KNOWLEDGE_BASE_PATH, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_knowledge_base(self):
        """Сохранение базы знаний в файл"""
        with open(KNOWLEDGE_BASE_PATH, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)

    def process_first_chunk(self, text):
        """Обработка первого чанка"""
        prompt = f"""Извлеки данные из юридического/публицистического документа. Формат ответа строго соблюдай:
doc_name: [полное название документа]
doc_date: [дата в формате ГГГГ-ММ-ДД или "Не указана"]
doc_type: [тип: закон, указ, постановление, судебный акт, статья или иное]
chunk_summary: [три тезиса о содержании]
qa_pairs: [3 пары в формате "вопрос:: ответ" (разделитель - два двоеточия)]
chunk_keywords: [3 ключевых слова, 3 ключевых фразы]

Текст: {text[:5000]}..."""
        return self.send_llm_request(prompt)

    def process_chunk(self, text, context, is_last=False):
        """Обработка обычного или последнего чанка"""
        prompt = f"""Проанализируй фрагмент документа. Контекст: {'; '.join(context)}
{"Это последний фрагмент документа. " if is_last else ""}Формат ответа строго соблюдай:
chunk_summary: [три тезиса о содержании]
qa_pairs: [3 пары в формате "вопрос:: ответ" (разделитель - два двоеточия)]
{"chunk_keywords: [3 ключевых слова, 3 ключевых фразы]" if not is_last else ""}

Текст: {text[:5000]}..."""
        return self.send_llm_request(prompt)

    def generate_doc_summary(self, summaries):
        """Генерация общего саммари документа"""
        prompt = f"""Напиши краткое саммари всего документа на основе анализа его частей:
{' '.join(summaries)}

Саммари должно содержать 5-7 основных положений документа, быть связным текстом."""
        return self.send_llm_request(prompt).strip()

    def send_llm_request(self, prompt):
        """Отправка запроса к LLM API"""
        headers = {
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
        data = {
            "model": LLM,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 2000
        }

        try:
            response = requests.post(URL_API, json=data, headers=headers, timeout=60)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            print(f"Ошибка запроса к LLM: {str(e)}")
            raise

    def parse_llm_response(self, response, chunk_index):
        parsed = {
            'chunk_summary': 'Не удалось извлечь',
            'chunk_text': '',  # Будет заполнено позже
            'qa_pairs': [],
            'chunk_keywords': []
        }
        # ... остальная часть метода без изменений

        if chunk_index == 0:
            parsed.update({
                'doc_name': 'Неизвестно',
                'doc_date': 'Не указана',
                'doc_type': 'Неизвестен'
            })

        try:
            lines = [line.strip() for line in response.split('\n') if line.strip()]
            current_section = None

            for line in lines:
                # Обработка названия документа (только для первого чанка)
                if chunk_index == 0:
                    if line.startswith('doc_name:') and len(line.split(': ')) > 1:
                        parsed['doc_name'] = line.split(': ')[1].strip()
                    elif line.startswith('doc_date:') and len(line.split(': ')) > 1:
                        parsed['doc_date'] = line.split(': ')[1].strip()
                    elif line.startswith('doc_type:') and len(line.split(': ')) > 1:
                        parsed['doc_type'] = line.split(': ')[1].strip()

                # Обработка остальных полей
                if line.startswith('chunk_summary:'):
                    current_section = 'summary'
                    parsed['chunk_summary'] = line.split(': ')[1].strip() if len(line.split(': ')) > 1 else ''
                elif line.startswith('qa_pairs:'):
                    current_section = 'qa'
                elif line.startswith('chunk_keywords:'):
                    current_section = 'keywords'
                else:
                    if current_section == 'summary':
                        parsed['chunk_summary'] += ' ' + line
                    elif current_section == 'qa' and '::' in line:
                        parts = line.split('::', 1)
                        if len(parts) == 2:
                            parsed['qa_pairs'].append({
                                'question': parts[0].strip(),
                                'answer': parts[1].strip()
                            })
                    elif current_section == 'keywords':
                        if len(parsed['chunk_keywords']) < 6:
                            parsed['chunk_keywords'].append(line.strip())

            # Очистка и валидация данных
            parsed['chunk_summary'] = ' '.join(parsed['chunk_summary'].split())
            parsed['qa_pairs'] = parsed['qa_pairs'][:3] if len(parsed['qa_pairs']) > 3 else parsed['qa_pairs']
            parsed['chunk_keywords'] = parsed['chunk_keywords'][:6] if len(parsed['chunk_keywords']) > 6 else parsed['chunk_keywords']

        except Exception as e:
            print(f"Ошибка парсинга ответа LLM: {str(e)}")
            # Возвращаем значения по умолчанию в случае ошибки
            parsed['chunk_summary'] = f"Ошибка обработки: {str(e)}"

        return parsed

    def aggregate_keywords(self, chunks):
        """Агрегация ключевых слов из всех чанков"""
        keyword_counts = {}
        for chunk in chunks:
            for keyword in chunk.get('chunk_keywords', []):
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1

        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:10]]

# Создаем и запускаем процессор
processor = DocumentProcessor()