# -*- coding: utf-8 -*-
import streamlit as st
import os
import re
import json  # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç json
import io
from docx import Document
from rank_bm25 import BM25Okapi
import numpy as np
from collections import defaultdict
import requests
from typing import List, Dict, Any
from rake_nltk import Rake
from pymorphy2 import MorphAnalyzer

from pathlib import Path

import json
import os
import re
from pathlib import Path

import json
import os
import re

import shutil

def safe_read_json(file_path: str) -> dict:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ JSON —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º"""
    try:
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
        backup_path = str(Path(file_path).with_suffix('.bak'))
        shutil.copy2(file_path, backup_path)
        
        # –ß—Ç–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π BOM
        with open(file_path, 'rb') as f:
            content = f.read().decode('utf-8-sig')
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        content = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', content)
        content = content.replace('\\u0002', '')
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON —á–∞—Å—Ç–∏
        start = content.find('{')
        end = content.rfind('}') + 1
        
        if start == -1 or end == 0:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã JSON-—Å–∫–æ–±–∫–∏")
            
        return json.loads(content[start:end])
        
    except Exception as e:
        st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON:")
        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
        if os.path.exists(backup_path):
            try:
                with open(backup_path, 'rb') as f:
                    return json.loads(f.read().decode('utf-8-sig'))
            except Exception:
                pass
        raise

try:
    from config import API_KEY, API_URL
except ImportError:
    st.error("–û—à–∏–±–∫–∞: –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª config.py —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ API_KEY –∏ API_URL")
    API_KEY = ""
    API_URL = "https://api.vsegpt.ru/v1/chat/completions"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DATA_DIR = "data"
MAX_CONTEXT_LENGTH = 15000
MAX_ANSWER_LENGTH = 15000
TEMPERATURE = 0.2
os.makedirs(DATA_DIR, exist_ok=True)

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
SYSTEM_PROMPT = """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –∏–∑ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –¢—ã –º–æ–∂–µ—à—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å
–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Ä–æ–≤. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã, –ø—É–Ω–∫—Ç—ã, –ø—Ä–∞–≤–∏–ª–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã,
–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

BUTTON_PROMPTS = {
    "quality": """–¢—ã —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ù–ê –û–°–ù–û–í–ï –≠–¢–ò–• –§–†–ê–ì–ú–ï–ù–¢–û–í:\n{context}\n\n. –ü—Ä–∞–≤–∏–ª–∞ –∞–Ω–∞–ª–∏–∑–∞: 
1. –ü–æ–ª–Ω–æ—Ç—É, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏, 
2. –æ—Ç–Ω–æ—Å–∏–º–æ—Å—Ç—å –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, 
3. –ø–æ–ª–Ω–æ—Ç—É —É–∫–∞–∑–∞–Ω–∏—è –≤ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω–∏–º—ã—Ö –Ω–æ—Ä–º –ø—Ä–∞–≤–∞. 
4. —Å—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 1 –¥–æ 10 ["–Ø –±—ã –ø–æ—Å—Ç–∞–≤–∏–ª –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –¥–æ–∫—É–º–µ–Ω—Ç—É ... –∏–∑ 10]
5. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
6.[–í–ê–ñ–ù–û] –∫—Ä–∏—Ç–∏—á–Ω–æ –æ—Ü–µ–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–º —É–∫–∞–∑–∞–Ω–∏—è–º –ø–æ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤  \n{context}\n\n  (–µ—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ), –ø—Ä–∏ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–π - –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –Ω–∞—Ä—É—à–µ–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π, –µ—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.""",

    "strategy": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ù–∞–ø–∏—à–∏ –ø–æ—à–∞–≥–æ–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑, –∏—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, —Å–º–µ–Ω—ã –∞–∫—Ü–µ–Ω—Ç–æ–≤ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–∏, –Ω–∞ —á–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è: –Ω–∞ —Å–±–æ—Ä–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –∏–ª–∏ —Å–º–µ–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.""",

    "prediction": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏ —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç —É—Å–∏–ª–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ —á–∞—Å—Ç–∏‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–æ–≤–µ—Ä–≥–∞—Ç—å‚Ä¶], —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏, –∫–∞–∫–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å—Ö–æ–¥ –¥–µ–ª–∞ –ø—Ä–∏ –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ–º –¥–ª—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Å–∏—Ç—É–∞—Ü–∏–∏."""
}

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')

    def preprocess(self, text: str) -> List[str]:
        text = self.regex.sub(' ', text.lower())
        return text.split()

class BM25SearchEngine:
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.bm25 = None
        self.chunks_info = []
        self.is_index_loaded = False
        self.cache_path = os.path.join("data", "bm25_index.json")
        self._load_index()  # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É
        
       



    def _load_index(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        if not os.path.exists(self.cache_path):
            st.sidebar.error("–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return False
    
        try:
            with open(self.cache_path, 'rb') as f:
                content = f.read().decode('utf-8-sig')
        
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
            if not content.strip().startswith('{'):
                return False
        
            data = json.loads(content)
        
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if not isinstance(data, dict) or 'metadata' not in data:
                return False
        
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            processed_texts = []
            for item in data.get('metadata', []):
                processed = item.get('processed', [])
                if isinstance(processed, list):
                    processed = ' '.join(processed)  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞—Å—Å–∏–≤ –≤ —Å—Ç—Ä–æ–∫—É
                processed = self._normalize_processed(processed)
                if processed:
                    tokens = processed.split()
                    if tokens:
                        processed_texts.append(tokens)
            if not self.chunks_info:
                st.sidebar.info("–ò–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞ –ø—É—Å—Ç. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.")
                return []
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
            if not processed_texts or all(len(tokens) == 0 for tokens in processed_texts):
                st.sidebar.info("–û—à–∏–±–∫–∞: –Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞.")
                return False
        
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25 —Å –Ω–µ–ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º —Ç–æ–∫–µ–Ω–æ–≤
            self.bm25 = BM25Okapi(processed_texts)
            self.chunks_info = data['metadata']
            self.is_index_loaded = True
            st.sidebar.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω BM25 —Å {len(processed_texts)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.")
            return True
        
        except Exception as e:
            st.sidebar.info(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return False

#    def _create_empty_index(self):
#        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
#        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BM25 —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º, –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
#        self.bm25 = BM25Okapi([])  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º
#        self.chunks_info = []
#        self.is_index_loaded = True

    def search(self, query: str, top_n: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        print(f"–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {query}")  # Debug
        if not self.is_index_loaded:
            print("–ò–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω!")  # Debug
            return []

        try:
            tokens = self.preprocessor.preprocess(query)
            if not tokens:
                return []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –∏–Ω–¥–µ–∫—Å –Ω–µ –ø—É—Å—Ç
            if not hasattr(self.bm25, 'doc_freqs') or len(self.bm25.doc_freqs) == 0:
                return []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            if len(self.bm25.doc_len) == 0:
                return []

            scores = self.bm25.get_scores(tokens)
            if scores is None or len(scores) == 0:
                return []

            best_indices = np.argsort(scores)[-top_n:][::-1]

            return [
                {
                    'doc_id': self.chunks_info[idx].get('file_id', ''),
                    'doc_name': self.chunks_info[idx].get('doc_name', '–î–æ–∫—É–º–µ–Ω—Ç'),
                    'chunk_text': self.chunks_info[idx].get('original', '')[:1000],
                    'score': round(float(scores[idx]), 4)
                }
                for idx in best_indices
                if idx < len(self.chunks_info)
            ]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []            
class LLMClient:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        self.is_initialized = True

    def query(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        try:
            payload = {
                "model": "google/gemini-2.0-flash-lite-001",
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }

            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                
                raise Exception(f"API Error {response.status_code}: {error_detail}")

            return response.json()['choices'][0]['message']['content']
        except requests.exceptions.RequestException as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}")
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")

class DocumentAnalyzer:
    def __init__(self, api_url: str = None, api_key: str = None):
        self.preprocessor = TextPreprocessor()
        self.search_engine = BM25SearchEngine(self.preprocessor)
        self.current_docx = None  # –¢–µ–∫—É—â–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π DOCX-–¥–æ–∫—É–º–µ–Ω—Ç
        self.llm_client = None
        self.llm_initialized = False
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM
        self._initialize_llm(api_url, api_key)

    def _initialize_llm(self, api_url: str, api_key: str) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç LLM"""
        if not api_url or not api_key:
            self.llm_initialized = False
            return
            
        try:
            self.llm_client = LLMClient(api_url, api_key)
            self.llm_initialized = True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}")
            self.llm_initialized = False

    def analyze_document(self, prompt_type: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
        if not self.current_docx:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ DOCX —Ñ–∞–π–ª"
            
        # 1. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ DOCX
        docx_text = self.current_docx["content"]
        
        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è BM25 –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ DOCX
        query = self._generate_search_query(prompt_type, docx_text)
        
        # 3. –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å–µ BM25
        chunks = self.search_engine.search(query)
        
        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context = self._build_context(docx_text, chunks)
        
        # 5. –§–æ—Ä–º–∏—Ä—É–µ–º –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": BUTTON_PROMPTS[prompt_type] + f"\n\n–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {st.session_state.doc_weight_slider:.1f}\n\n–ö–û–ù–¢–ï–ö–°–¢:\n" + context}
        ]

        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≤ —Å–∞–π–¥–±–∞—Ä
        st.sidebar.header("–ò—Ç–æ–≥–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM")
        st.sidebar.markdown("### –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:")
        st.sidebar.markdown(BUTTON_PROMPTS[prompt_type] + f"\n\n–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {st.session_state.doc_weight_slider:.1f}\n\n–ö–û–ù–¢–ï–ö–°–¢:\n" + context)
        
        return self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)

    def load_documents(self, uploaded_files) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DOCX —Ñ–∞–π–ª—ã"""
        if not uploaded_files:
            return

        try:
            for uploaded_file in uploaded_files:
                try:
                    if uploaded_file.size == 0:
                        st.warning(f"–§–∞–π–ª {uploaded_file.name} –ø—É—Å—Ç")
                        continue

                    file_bytes = io.BytesIO(uploaded_file.read())
                    doc = Document(file_bytes)
                    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

                    if not text:
                        st.warning(f"–§–∞–π–ª {uploaded_file.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
                        continue

                    if len(text) > MAX_CONTEXT_LENGTH:
                        text = text[:MAX_CONTEXT_LENGTH]
                        st.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {uploaded_file.name} –æ–±—Ä–µ–∑–∞–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                    self.current_docx = {
                        "name": uploaded_file.name,
                        "content": text
                    }

                    st.success(f"–î–æ–∫—É–º–µ–Ω—Ç {uploaded_file.name} –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {uploaded_file.name}: {str(e)}")
                    continue

        except Exception as e:
            st.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")

    def _generate_search_query(self, prompt_type: str, docx_text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è BM25"""
        base_queries = {
            "quality": "–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞",
            "strategy": "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–ø–æ—Ä–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ",
            "prediction": "–ø–æ–∑–∏—Ü–∏—è –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞"
        }
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º DOCX
        return f"{base_queries[prompt_type]} {docx_text[:1000]}"

    def _build_context(self, docx_text: str, chunks: List[Dict]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM –∏–∑ DOCX –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"""
        context_parts = [
            "=== –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ô –î–û–ö–£–ú–ï–ù–¢ ===",
            docx_text,
            "\n=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô ==="
        ]
        
        for chunk in chunks:
            context_parts.append(f"\nüìÑ {chunk['doc_name']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk['score']:.2f}):")
            context_parts.append(chunk['chunk_text'][:3000])
        
        return "\n".join(context_parts)

def main():
    st.set_page_config(page_title="El Documente", layout="wide")
    gif_path = "data/maracas-sombrero-hat.gif"
    st.image(gif_path, caption="Hola!", width=64)
    st.title("El Documente: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = DocumentAnalyzer(API_URL, API_KEY)
    
    analyzer = st.session_state.analyzer
    
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    col1, col2 = st.sidebar.columns([3, 1])
    with col1:
        weight = st.slider(
            "–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
            0.1, 1.0, 0.7, 0.1,
            key="doc_weight_slider",
            help="–†–µ–≥—É–ª–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"
        )

    with col2:
        st.metric("–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", f"{weight:.1f}")

    st.sidebar.write(f"–í—ã–±—Ä–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ: {weight}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM
    if not analyzer.llm_initialized:
        st.sidebar.error("LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á –∏ URL")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    uploaded_files = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ DOCX", 
        type=["docx"], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤..."):
            analyzer.load_documents(uploaded_files)
        st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(uploaded_files)}")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —á–∞—Ç–∞
    CHAT_TEMPERATURE = 0.6
    CHAT_SYSTEM_PROMPT = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —é—Ä–∏—Å—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –ø—Ä–∞–≤–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ó–ê–ü–†–ï–©–ï–ù–û:  1. —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –∏ —Å—É–¥–µ–±–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É. 2. —É–∫–∞–∑—ã–≤–∞—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ, —á—Ç–æ —Ç—ã –æ–∑–Ω–∞–∫–æ–º–∏–ª—Å—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º, –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥–∏–∞–ª–æ–≥. –û—Ç–≤–µ—Ç—ã –∏–∑–ª–∞–≥–∞–π –≤ –¥–µ–ª–æ–≤–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏—Ö –º–Ω–µ–Ω–∏–π."""

    # –ß–∞—Ç —Å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º –ö–∞—Ä–ª–æ—Å–æ–º
    st.header("–ß–∞—Ç")

    user_input = st.text_area(
        "–û–±—Å—É–¥–∏—Ç—å —Å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º –ö–∞—Ä–ª–æ—Å–æ–º",
        max_chars=500,
        height=100
    )

    ask_button = st.button("–°–ø—Ä–æ—Å–∏—Ç—å", disabled=not (uploaded_files and user_input))

    if 'docx_added' not in st.session_state:
        st.session_state.docx_added = False

    if ask_button:
        conversation_log = []

        if not st.session_state.docx_added and analyzer.current_docx:
            conversation_log.append(analyzer.current_docx["content"])
            st.session_state.docx_added = True

        conversation_log.append(user_input)

        relevant_chunks = analyzer.search_engine.search(user_input)

        for chunk in relevant_chunks:
            conversation_log.append(chunk['chunk_text'])

        messages = [
            {"role": "system", "content": CHAT_SYSTEM_PROMPT},
            {"role": "user", "content": "\n".join(conversation_log)}
        ]

        response = analyzer.llm_client.query(messages, CHAT_TEMPERATURE, MAX_ANSWER_LENGTH)

        response_container = st.empty()
        response_container.text_area("–û—Ç–≤–µ—Ç –æ—Ç –ö–∞—Ä–ª–æ—Å–∞", value=response, height=200, disabled=True)

    # –ö–Ω–æ–ø–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
    st.header("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    col1, col2, col3 = st.columns(3)

    buttons_disabled = not (uploaded_files and analyzer.llm_initialized)
    
    with col1:
        if st.button("–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞", disabled=buttons_disabled):
            with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞..."):
                result = analyzer.analyze_document("quality")
                st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏")
                st.markdown(result)
    
    with col2:
        if st.button("–î–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–ø–æ—Ä–∞", disabled=buttons_disabled):
            with st.spinner("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π..."):
                result = analyzer.analyze_document("strategy")
                st.markdown("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
                st.markdown(result)
    
    with col3:
        if st.button("–°–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã", disabled=buttons_disabled):
            with st.spinner("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏..."):
                result = analyzer.analyze_document("prediction")
                st.markdown("### –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞")
                st.markdown(result)

if __name__ == "__main__":
    main()
