# -*- coding: utf-8 -*-
import streamlit as st
import os
import re
import json
import io
from docx import Document
from rank_bm25 import BM25Okapi
import numpy as np
from collections import defaultdict
import requests
from typing import List, Dict, Any
import shutil
from pathlib import Path
import glob
from json import JSONDecodeError

try:
    from config import API_KEY, API_URL
except ImportError:
    st.error("–û—à–∏–±–∫–∞: –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª config.py —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ API_KEY –∏ API_URL")
    API_KEY = ""
    API_URL = "https://api.vsegpt.ru/v1/chat/completions"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DATA_DIR = "data"
MAX_CONTEXT_LENGTH = 20000
MAX_ANSWER_LENGTH = 15000
TEMPERATURE = 0.2
os.makedirs(DATA_DIR, exist_ok=True)

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
KEYWORDS_PROMPT = """
–ò–∑–≤–ª–µ–∫–∏ 10-15 –∫–ª—é—á–µ–≤—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è:
- –ù–∞–∑–≤–∞–Ω–∏—è –∑–∞–∫–æ–Ω–æ–≤ (–ì–ö –†–§, –ö–æ–ê–ü –∏ —Ç.–¥.).
- –¢–∏–ø–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è ("–∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ", "–∞–ø–µ–ª–ª—è—Ü–∏–æ–Ω–Ω–∞—è –∂–∞–ª–æ–±–∞").
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è ("–Ω–µ—É—Å—Ç–æ–π–∫–∞", "–ø—Ä–æ—Å—Ä–æ—á–∫–∞ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è").
–§–æ—Ä–º–∞—Ç: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—è—Ç—ã–º–∏.
"""

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
SYSTEM_PROMPT = """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –∏–∑ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –¢—ã –º–æ–∂–µ—à—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å
–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Ä–æ–≤. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã, –ø—É–Ω–∫—Ç—ã, –ø—Ä–∞–≤–∏–ª–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã,
–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

BUTTON_PROMPTS = {
    "quality": """–¢—ã —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ù–ê –û–°–ù–û–í–ï –≠–¢–ò–• –§–†–ê–ì–ú–ï–ù–¢–û–í:\n{context}\n\n. –ü—Ä–∞–≤–∏–ª–∞ –∞–Ω–∞–ª–∏–∑–∞: 
    1. –ü–æ–ª–Ω–æ—Ç—É, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏, 
    2. –æ—Ç–Ω–æ—Å–∏–º–æ—Å—Ç—å –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, 
    3. –ø–æ–ª–Ω–æ—Ç—É —É–∫–∞–∑–∞–Ω–∏—è –≤ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω–∏–º—ã—Ö –Ω–æ—Ä–º –ø—Ä–∞–≤–∞. 
    4. —Å—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 1 –¥–æ 10 ["–Ø –±—ã –ø–æ—Å—Ç–∞–≤–∏–ª –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –¥–æ–∫—É–º–µ–Ω—Ç—É ... –∏–∑ 10]
    5. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    6.[–í–ê–ñ–ù–û] –∫—Ä–∏—Ç–∏—á–Ω–æ –æ—Ü–µ–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–º —É–∫–∞–∑–∞–Ω–∏—è–º –ø–æ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤  \n{context}\n\n  (–µ—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ), –ø—Ä–∏ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–π - –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –Ω–∞—Ä—É—à–µ–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π, –µ—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.""",

    "strategy": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ù–∞–ø–∏—à–∏ –ø–æ—à–∞–≥–æ–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑, –∏—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, —Å–º–µ–Ω—ã –∞–∫—Ü–µ–Ω—Ç–æ–≤ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–∏, –Ω–∞ —á–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è: –Ω–∞ —Å–±–æ—Ä–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –∏–ª–∏ —Å–º–µ–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.""",

    "prediction": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏ —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç —É—Å–∏–ª–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ —á–∞—Å—Ç–∏‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–æ–≤–µ—Ä–≥–∞—Ç—å‚Ä¶], —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏, –∫–∞–∫–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å—Ö–æ–¥ –¥–µ–ª–∞ –ø—Ä–∏ –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ–º –¥–ª—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Å–∏—Ç—É–∞—Ü–∏–∏."""
}

def merge_json_parts(base_filename: str) -> dict:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —á–∞—Å—Ç–∏ JSON —Ñ–∞–π–ª–æ–≤ –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å –≤ –ø–∞–º—è—Ç–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –Ω–æ–º–µ—Ä–æ–≤ —á–∞—Å—Ç–µ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        base_name = re.sub(r'_part\d+', '', base_filename)
        base_name = re.sub(r'\.json$', '', base_name)
        
        pattern = os.path.join(DATA_DIR, f"{base_name}_part*.json")
        part_files = glob.glob(pattern)
        
        if not part_files:
            print(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –ø–æ —à–∞–±–ª–æ–Ω—É: {pattern}")
            return None
        
        def get_part_number(filename):
            match = re.search(r'_part(\d+)\.json$', filename)
            return int(match.group(1)) if match else 0
            
        part_files = sorted(part_files, key=get_part_number)
        print(f"–ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (–≤ –ø–æ—Ä—è–¥–∫–µ): {part_files}")
        
        merged_data = {'metadata': [], 'processed_files': []}
        success_count = 0
        
        for part_file in part_files:
            try:
                print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {part_file}")
                part_data = safe_read_json(part_file)
                
                if not part_data:
                    print(f"–§–∞–π–ª {part_file} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—á–∏—Ç–∞–Ω")
                    continue
                    
                if 'metadata' in part_data and isinstance(part_data['metadata'], list):
                    merged_data['metadata'].extend(part_data['metadata'])
                    success_count += 1
                    
                if 'processed_files' in part_data and isinstance(part_data['processed_files'], list):
                    merged_data['processed_files'].extend(part_data['processed_files'])
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {part_file}: {str(e)}")
                continue
        
        if not merged_data['metadata']:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö metadata –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
            return None
            
        merged_data['processed_files'] = list(set(merged_data['processed_files']))
        print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {success_count}/{len(part_files)} —Ñ–∞–π–ª–æ–≤")
        
        return merged_data
        
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ JSON —á–∞—Å—Ç–µ–π: {str(e)}")
        return None

def safe_read_json(file_path: str) -> dict:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ JSON —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        backup_path = str(Path(file_path).with_suffix('.bak'))
        shutil.copy2(file_path, backup_path)
        
        with open(file_path, 'rb') as f:
            content_bytes = f.read()
        
        try:
            content = content_bytes.decode('utf-8-sig')
        except UnicodeDecodeError:
            try:
                content = content_bytes.decode('cp1251')
            except UnicodeDecodeError:
                content = content_bytes.decode('latin-1')
        
        content = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', content)
        
        if content.startswith('\ufeff'):
            content = content[1:]
        
        start = content.find('{')
        end = content.rfind('}') + 1
        
        if start == -1 or end == 0:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã JSON-—Å–∫–æ–±–∫–∏")
        
        json_content = content[start:end]
        json_content = re.sub(r',\s*([}\]])', r'\1', json_content)
        
        try:
            return json.loads(json_content)
        except json.JSONDecodeError as e:
            try:
                last_brace = json_content.rfind('}')
                if last_brace != -1:
                    json_content = json_content[:last_brace+1]
                
                first_brace = json_content.find('{')
                if first_brace != -1:
                    json_content = json_content[first_brace:]
                
                return json.loads(json_content)
            except json.JSONDecodeError:
                if os.path.exists(backup_path):
                    with open(backup_path, 'rb') as f:
                        backup_content = f.read().decode('utf-8-sig')
                        return json.loads(backup_content)
                raise
                
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
        return None

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')

    def preprocess(self, text: str) -> List[str]:
        text = self.regex.sub(' ', text.lower())
        return text.split()

class BM25SearchEngine:
    def __init__(self, preprocessor):
        self.preprocessor = preprocessor
        self.bm25 = None
        self.chunks_info = []
        self.is_index_loaded = False
        self.llm_keywords = []
        self.data_dir = "data"
        self._load_index()
        self.min_score = 0.15  # –ù–æ–≤—ã–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

    def _find_part_files(self):
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –∏–Ω–¥–µ–∫—Å–∞ —Å part –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data"""
        try:
            if not os.path.exists(self.data_dir):
                os.makedirs(self.data_dir, exist_ok=True)
                return []

            part_files = []
            for filename in os.listdir(self.data_dir):
                if "part" in filename.lower() and filename.endswith(".json"):
                    full_path = os.path.join(self.data_dir, filename)
                    part_files.append(full_path)

            def extract_part_number(f):
                match = re.search(r'_part(\d+)\.json$', f, re.IGNORECASE)
                return int(match.group(1)) if match else 0

            return sorted(part_files, key=extract_part_number)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
            return []

    def _normalize_text(self, text_data):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        STOP_ORGANIZATIONS = [
            "–ü–ê–û –¢ –ü–ª—é—Å", "–ê–û –ï–¢–ö", "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥—Å–∫–∞—è —Ç–µ–ø–ª–æ—Å–µ—Ç–µ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è",
            "–¢ –ü–ª—é—Å", "–ï–¢–ö", "AO ETK"
        ]
    
        if not text_data:
            return []

        if isinstance(text_data, str):
            for org in STOP_ORGANIZATIONS:
                text_data = text_data.replace(org, "")
            
            text_data = re.sub(r'\S+@\S+', '', text_data)
            text_data = re.sub(r'[\+\(\)\d\-]{6,}', '', text_data)
        
            return self.preprocessor.preprocess(text_data)

        return []

    def _read_json_with_recovery(self, file_path):
        """–ß—Ç–µ–Ω–∏–µ JSON —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read().decode('utf-8-sig')

            content = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', content)
            
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                start = content.find('{')
                end = content.rfind('}') + 1
                if start >= 0 and end > 0:
                    return json.loads(content[start:end])
                raise
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
            raise

    def _load_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ —á–∞—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ–ª–µ original"""
        try:
            part_files = self._find_part_files()
            if not part_files:
                print("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
                return False

            merged_data = {'metadata': [], 'processed_files': set()}
            
            for file_path in part_files:
                try:
                    file_data = self._read_json_with_recovery(file_path)
                    if not file_data:
                        continue

                    if 'metadata' in file_data and isinstance(file_data['metadata'], list):
                        for item in file_data['metadata']:
                            if isinstance(item, dict) and 'original' in item:
                                # –ò—Å–ø–æ–ª—å–∑—É–µ–º original —Ç–µ–∫—Å—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                                item['processed'] = self._normalize_text(item['original'])
                                merged_data['metadata'].append(item)

                    if 'processed_files' in file_data and isinstance(file_data['processed_files'], list):
                        merged_data['processed_files'].update(file_data['processed_files'])

                except Exception as e:
                    st.sidebar.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
                    continue

            if not merged_data['metadata']:
                st.sidebar.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞")
                return False

            corpus = []
            valid_metadata = []
            
            for item in merged_data['metadata']:
                processed = item.get('processed', [])
                if processed:
                    corpus.append(processed)
                    valid_metadata.append(item)

            if not corpus:
                st.sidebar.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
                return False

            self.bm25 = BM25Okapi(corpus, k1=1.5, b=0.75)
            self.chunks_info = valid_metadata
            self.is_index_loaded = True
            st.sidebar.info(f"–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(corpus)}")
            return True

        except Exception as e:
            st.sidebar.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
            return False



    def search(self, keywords: List[str], top_n=10):
        if not self.is_index_loaded or not keywords:
            return []

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        tokens = list(set([word.lower() for keyword in keywords 
                         for word in self.preprocessor.preprocess(keyword) 
                         if len(word) > 2]))
        
        if not tokens:
            return []

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫
        scores = self.bm25.get_scores(tokens)
        max_score = max(scores) if scores else 0
        normalized_scores = scores / max_score if max_score > 0 else scores

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
        results = []
        for idx, score in enumerate(normalized_scores):
            if score >= self.min_score and idx < len(self.chunks_info):
                results.append({
                    'doc_id': self.chunks_info[idx].get('file_id', ''),
                    'doc_name': self.chunks_info[idx].get('doc_name', '–î–æ–∫—É–º–µ–Ω—Ç'),
                    'chunk_text': self.chunks_info[idx].get('original', ''),
                    'score': round(float(score), 4)
                })

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
        results = sorted(results, key=lambda x: x['score'], reverse=True)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        seen_texts = set()
        filtered_results = []
        for res in results:
            text_hash = hash(res['chunk_text'][:500])
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                filtered_results.append(res)
        
        return filtered_results[:top_n]
class LLMClient:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        self.is_initialized = True

    def query(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        try:
            payload = {
                "model": "google/gemini-2.0-flash-lite-001",
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }

            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                
                raise Exception(f"API Error {response.status_code}: {error_detail}")

            return response.json()['choices'][0]['message']['content']
        except requests.exceptions.RequestException as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}")
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")

class DocumentAnalyzer:
    def __init__(self, api_url: str = None, api_key: str = None):
        self.preprocessor = TextPreprocessor()
        self.search_engine = BM25SearchEngine(self.preprocessor)
        self.current_docx = None
        self.llm_client = None
        self.llm_initialized = False
        self._initialize_llm(api_url, api_key)

    def _initialize_llm(self, api_url: str, api_key: str) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç LLM"""
        if not api_url or not api_key:
            self.llm_initialized = False
            return
            
        try:
            self.llm_client = LLMClient(api_url, api_key)
            self.llm_initialized = True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}")
            self.llm_initialized = False

    def _generate_keywords_from_text(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.llm_initialized or not text:
            return []
            
        try:
            messages = [
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏. –û–ø—Ä–µ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤–æ —Å–ø–æ—Ä–∞, –ø–æ–¥–≥–æ—Ç–æ–≤—å –Ω–æ —Å–∞–º–º–∞—Ä–∏ —Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–æ–≤ –∏ —Å–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫ –∏–∑ 10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–æ–¥–∏–Ω —Å–∏–Ω–æ–Ω–∏–º –Ω–∞ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ) –∏ –¥–µ—Å—è—Ç–∏ —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã—Ö –ø–æ —Å–º—ã—Å–ª—É –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –ø–æ —ç—Ç–æ–º—É —Å–∞–º–º–∞—Ä–∏."},
                {"role": "user", "content": f"{KEYWORDS_PROMPT}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{text[:10000]}"}
            ]
            
            response = self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)
            
            keywords = []
            for line in response.split('\n'):
                if '‚Üí' in line:
                    parts = line.split('‚Üí')
                    for part in parts:
                        keywords.extend(re.findall(r'[\w\-]+', part.strip()))
                else:
                    keywords.extend(re.findall(r'[\w\-]+', line.strip()))
            
            keywords = list(set(k.lower() for k in keywords if k.strip()))
            return keywords
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
            return []

    def analyze_document(self, prompt_type: str) -> str:
        if not self.current_docx:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ DOCX —Ñ–∞–π–ª"
            
        docx_text = self.current_docx["content"]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if not self.search_engine.llm_keywords:
            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤..."):
                keywords = self._generate_keywords_from_text(docx_text)
                if not keywords:
                    return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"
                self.search_engine.llm_keywords = keywords
                st.sidebar.info(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords)}")
        
        # –ü–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        chunks = self.search_engine.search(self.search_engine.llm_keywords)
        context = self._build_context(docx_text, chunks)
        
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": BUTTON_PROMPTS[prompt_type] + f"\n\n–ö–û–ù–¢–ï–ö–°–¢:\n" + context}
        ]

        st.sidebar.header("–ò—Ç–æ–≥–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM")
        st.sidebar.markdown("### –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:")
        st.sidebar.markdown(BUTTON_PROMPTS[prompt_type] + f"\n\n–ö–û–ù–¢–ï–ö–°–¢:\n" + context)
        
        return self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)

    def load_documents(self, uploaded_files) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DOCX —Ñ–∞–π–ª—ã"""
        if not uploaded_files:
            return

        try:
            for uploaded_file in uploaded_files:
                try:
                    if uploaded_file.size == 0:
                        st.warning(f"–§–∞–π–ª {uploaded_file.name} –ø—É—Å—Ç")
                        continue

                    file_bytes = io.BytesIO(uploaded_file.read())
                    doc = Document(file_bytes)
                    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

                    if not text:
                        st.warning(f"–§–∞–π–ª {uploaded_file.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
                        continue

                    if len(text) > MAX_CONTEXT_LENGTH:
                        text = text[:MAX_CONTEXT_LENGTH]
                        st.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {uploaded_file.name} –æ–±—Ä–µ–∑–∞–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤")

                    self.current_docx = {
                        "name": uploaded_file.name,
                        "content": text
                    }

                    st.success(f"–î–æ–∫—É–º–µ–Ω—Ç {uploaded_file.name} –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {uploaded_file.name}: {str(e)}")
                    continue

        except Exception as e:
            st.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")

    def _build_context(self, docx_text: str, chunks: List[Dict]) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        context_parts = ["=== –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ô –î–û–ö–£–ú–ï–ù–¢ ===", docx_text.strip()]
        
        if not chunks:
            return "\n".join(context_parts + ["\n=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´ –ù–ï –ù–ê–ô–î–ï–ù–´ ==="])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
        filtered_chunks = [c for c in chunks if c.get('score', 0) >= 0.1]
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        st.sidebar.markdown("**–û—Ç–ª–∞–¥–∫–∞ –ø–æ–∏—Å–∫–∞:**")
        st.sidebar.json({
            "keywords": self.search_engine.llm_keywords,
            "found_chunks": len(filtered_chunks),
            "top_scores": [c['score'] for c in filtered_chunks[:3]]
        })
        
        total_length = len(docx_text)
        MAX_CONTEXT = 18000
        
        for i, chunk in enumerate(filtered_chunks[:3]):  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-3
            chunk_text = chunk.get('chunk_text', '')
            doc_name = chunk.get('doc_name', '–î–æ–∫—É–º–µ–Ω—Ç')
            score = chunk.get('score', 0)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –Ω—É–ª–µ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
            if score <= 0:
                continue
                
            header = f"\nüìå **–§—Ä–∞–≥–º–µ–Ω—Ç {i+1}** ({doc_name}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}):\n"
            
            if total_length + len(header) + len(chunk_text) > MAX_CONTEXT:
                available = MAX_CONTEXT - total_length - len(header)
                if available > 50:
                    context_parts.append(header + chunk_text[:available] + "...")
                break
                
            context_parts.append(header + chunk_text)
            total_length += len(header) + len(chunk_text)
        
        return "\n".join(context_parts)

def main():
    st.set_page_config(page_title="El Documente", layout="wide", initial_sidebar_state="collapsed")
    gif_path = "data/maracas-sombrero-hat.gif"
    st.image(gif_path, caption="Hola!", width=64)
    st.title("El Documente: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
    
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = DocumentAnalyzer(API_URL, API_KEY)
    
    analyzer = st.session_state.analyzer
    
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    
    if not analyzer.llm_initialized:
        st.sidebar.error("LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á –∏ URL")
    
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    uploaded_files = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ DOCX", 
        type=["docx"], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤..."):
            analyzer.load_documents(uploaded_files)
        st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(uploaded_files)}")

    CHAT_TEMPERATURE = 0.6
    CHAT_SYSTEM_PROMPT = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —é—Ä–∏—Å—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –ø—Ä–∞–≤–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ó–ê–ü–†–ï–©–ï–ù–û:  1. —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –∏ —Å—É–¥–µ–±–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É. 2. —É–∫–∞–∑—ã–≤–∞—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ, —á—Ç–æ —Ç—ã –æ–∑–Ω–∞–∫–æ–º–∏–ª—Å—è —Å –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥–∏–∞–ª–æ–≥. –û—Ç–≤–µ—Ç—ã –∏–∑–ª–∞–≥–∞–π –≤ –¥–µ–ª–æ–≤–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏—Ö –º–Ω–µ–Ω–∏–π."""

    st.header("–ß–∞—Ç")

    user_input = st.text_area(
        "–û–±—Å—É–¥–∏—Ç—å —Å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º –ö–∞—Ä–ª–æ—Å–æ–º",
        max_chars=500,
        height=100
    )

    ask_button = st.button("–°–ø—Ä–æ—Å–∏—Ç—å", disabled=not (uploaded_files))

    if 'docx_added' not in st.session_state:
        st.session_state.docx_added = False

    if ask_button:
        doc_summary = analyzer.current_docx["content"][:3000]
        relevant_chunks = analyzer.search_engine.search(user_input)
    
        messages = [
            {"role": "system", "content": CHAT_SYSTEM_PROMPT},
            {"role": "assistant", "content": f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (—Å–æ–∫—Ä–∞—â—ë–Ω–Ω–æ):\n{doc_summary}"},
            *[
                {"role": "assistant", "content": f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç ({chunk['doc_name']}):\n{chunk['chunk_text'][:800]}"}
                for chunk in relevant_chunks[:2]
            ],
            {"role": "user", "content": f"–î–∏–∞–ª–æ–≥:\n{'\n'.join(st.session_state.get('chat_history', []))[-2:]}"},
            {"role": "user", "content": user_input}
        ]
    
        response = analyzer.llm_client.query(messages, temperature=0.7, max_tokens=1500)
        response_container = st.empty()
        response_container.markdown("### –û—Ç–≤–µ—Ç –æ—Ç –ö–∞—Ä–ª–æ—Å–∞")
        response_container.markdown(response)
        st.session_state.setdefault('chat_history', []).extend([user_input, response])


    st.header("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    col1, col2, col3 = st.columns(3)

    buttons_disabled = not (uploaded_files and analyzer.llm_initialized)
    
    with col1:
        if st.button("–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞", disabled=buttons_disabled):
            with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞..."):
                result = analyzer.analyze_document("quality")
                st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏")
                st.markdown(result)
    
    with col2:
        if st.button("–î–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–ø–æ—Ä–∞", disabled=buttons_disabled):
            with st.spinner("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π..."):
                result = analyzer.analyze_document("strategy")
                st.markdown("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
                st.markdown(result)
    
    with col3:
        if st.button("–°–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã", disabled=buttons_disabled):
            with st.spinner("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏..."):
                result = analyzer.analyze_document("prediction")
                st.markdown("### –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞")
                st.markdown(result)

if __name__ == "__main__":
    main()
