# -*- coding: utf-8 -*-
import streamlit as st
import os
import re
import json
import io
from docx import Document
from rank_bm25 import BM25Okapi
import numpy as np
from collections import defaultdict
import requests
from typing import List, Dict, Any
import shutil
from pathlib import Path
import glob

try:
    from config import API_KEY, API_URL
except ImportError:
    st.error("–û—à–∏–±–∫–∞: –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª config.py —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ API_KEY –∏ API_URL")
    API_KEY = ""
    API_URL = "https://api.vsegpt.ru/v1/chat/completions"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DATA_DIR = "data"
MAX_CONTEXT_LENGTH = 15000
MAX_ANSWER_LENGTH = 15000
TEMPERATURE = 0.2
os.makedirs(DATA_DIR, exist_ok=True)

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
KEYWORDS_PROMPT = """
–ó–∞–¥–∞—á–∞:
–ü—Ä–µ–æ–±—Ä–∞–∑—É–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É –¥–ª—è BM25-–ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –æ–±—Ä–∞–∑—Ü–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º. –£—á–∏—Ç—ã–≤–∞–π –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π –ª–µ–∫—Å–∏–∫–∏ –∏ —Å–ª–µ–¥—É—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:

–°–∞–º–º–∞—Ä–∏:
–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º, –æ —á–µ–º –¥–æ–∫—É–º–µ–Ω—Ç.

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ:
–ö –¥–µ—Å—è—Ç–∏ —Å–∞–º—ã–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–º –≤ –∑–∞–ø—Ä–æ—Å–µ —Å–ª–æ–≤–∞–º –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º –≤ –∑–∞–ø—Ä–æ—Å–µ –¥–æ–±–∞–≤—å –ø–æ 2-3 —Å–∏–Ω–æ–Ω–∏–º–∞/–±–ª–∏–∑–∫–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞/—Ç–æ–∂–¥–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–æ —Å–º—ã—Å–ª—É –≤—ã—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ò–õ–ò:
"—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞" ‚Üí "—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –ò–õ–ò –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –ò–õ–ò –∞–Ω–Ω—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞"
"—Ç–µ–ø–ª–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ" ‚Üí "–ø–æ—Å—Ç–∞–≤–∫–∞ —Ç–µ–ø–ª–æ–≤–æ–π —ç–Ω–µ—Ä–≥–∏–∏"

–£–∫–∞–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–∫–æ–Ω–æ–≤:
"–ì–ö –†–§" ‚Üí "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§ (–ì–ö –†–§)"

–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏—è:
–î–ª—è –æ–±—â–∏—Ö –ø–æ–Ω—è—Ç–∏–π –¥–æ–±–∞–≤—å –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É:
"–Ω–∞—Ä—É—à–µ–Ω–∏–µ —Å—Ä–æ–∫–æ–≤" ‚Üí "–ø—Ä–æ—Å—Ä–æ—á–∫–∞ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤ (—Å—Ç. 395 –ì–ö –†–§)"
–£–∫–∞–∂–∏ –±–ª–∏–∂–∞–π—à–∏–µ —Å–º–µ–∂–Ω—ã–µ –ø—Ä–∞–≤–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã:
"–Ω–µ—É—Å—Ç–æ–π–∫–∞" ‚Üí "–Ω–µ—É—Å—Ç–æ–π–∫–∞ (—à—Ç—Ä–∞—Ñ, –ø–µ–Ω—è)"

–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
–ù–ï –ò–ó–ú–ï–ù–Ø–ô –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π/–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
"—Å—Ç. 15.25 –ö–æ–ê–ü" ‚Üí "—Å—Ç–∞—Ç—å—è 15.25 –ö–æ–¥–µ–∫—Å–∞ –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö (–ö–æ–ê–ü)"
–°–æ—Ö—Ä–∞–Ω—è–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è:
"‚Ññ 127-–§–ó" ‚Üí "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ 127-–§–ó"

–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫:
–ò—Å–ø—Ä–∞–≤—å –æ—á–µ–≤–∏–¥–Ω—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏:
"–≠–ª–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç" ‚Üí "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"
–ü—Ä–µ–¥–ª–æ–∂–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤:
"–∏—Å–∫" ‚Üí "–∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ (–ò–°–ö) –ò–õ–ò –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Å—á–µ—Ç (–ò–ò–°)"

–ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∏–ª–∏ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:
–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (–≤–∫–ª—é—á–∞—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ)
–°–∏–Ω–æ–Ω–∏–º—ã —á–µ—Ä–µ–∑ "–ò–õ–ò"
–£—Ç–æ—á–Ω—è—é—â–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Å–∫–æ–±–∫–∞—Ö
–ù–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ–ª–Ω–æ–π —Ñ–æ—Ä–º–µ
"""

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
SYSTEM_PROMPT = """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –∏–∑ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –¢—ã –º–æ–∂–µ—à—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å
–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Ä–æ–≤. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã, –ø—É–Ω–∫—Ç—ã, –ø—Ä–∞–≤–∏–ª–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã,
–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

BUTTON_PROMPTS = {
    "quality": """–¢—ã —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ù–ê –û–°–ù–û–í–ï –≠–¢–ò–• –§–†–ê–ì–ú–ï–ù–¢–û–í:\n{context}\n\n. –ü—Ä–∞–≤–∏–ª–∞ –∞–Ω–∞–ª–∏–∑–∞: 
    1. –ü–æ–ª–Ω–æ—Ç—É, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏, 
    2. –æ—Ç–Ω–æ—Å–∏–º–æ—Å—Ç—å –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, 
    3. –ø–æ–ª–Ω–æ—Ç—É —É–∫–∞–∑–∞–Ω–∏—è –≤ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω–∏–º—ã—Ö –Ω–æ—Ä–º –ø—Ä–∞–≤–∞. 
    4. —Å—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 1 –¥–æ 10 ["–Ø –±—ã –ø–æ—Å—Ç–∞–≤–∏–ª –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –¥–æ–∫—É–º–µ–Ω—Ç—É ... –∏–∑ 10]
    5. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    6.[–í–ê–ñ–ù–û] –∫—Ä–∏—Ç–∏—á–Ω–æ –æ—Ü–µ–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–º —É–∫–∞–∑–∞–Ω–∏—è–º –ø–æ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤  \n{context}\n\n  (–µ—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ), –ø—Ä–∏ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–π - –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –Ω–∞—Ä—É—à–µ–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π, –µ—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.""",

    "strategy": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ù–∞–ø–∏—à–∏ –ø–æ—à–∞–≥–æ–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑, –∏—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, —Å–º–µ–Ω—ã –∞–∫—Ü–µ–Ω—Ç–æ–≤ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–∏, –Ω–∞ —á–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è: –Ω–∞ —Å–±–æ—Ä–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –∏–ª–∏ —Å–º–µ–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.""",

    "prediction": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏ —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç —É—Å–∏–ª–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ —á–∞—Å—Ç–∏‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–æ–≤–µ—Ä–≥–∞—Ç—å‚Ä¶], —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏, –∫–∞–∫–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å—Ö–æ–¥ –¥–µ–ª–∞ –ø—Ä–∏ –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ–º –¥–ª—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Å–∏—Ç—É–∞—Ü–∏–∏."""
}

def merge_json_parts(base_filename: str) -> dict:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —á–∞—Å—Ç–∏ JSON —Ñ–∞–π–ª–æ–≤ –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å –≤ –ø–∞–º—è—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å –±–∞–∑–æ–≤—ã–º –∏–º–µ–Ω–µ–º
        pattern = re.sub(r'_part\d+\.json$', '_part*.json', base_filename)
        part_files = sorted(glob.glob(pattern))
        
        if not part_files:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–±–∏—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª
            single_file = base_filename.replace('_part*.json', '.json')
            if os.path.exists(single_file):
                return safe_read_json(single_file)
            return None
        
        merged_data = {'metadata': [], 'processed_files': []}
        
        for part_file in part_files:
            part_data = safe_read_json(part_file)
            if not part_data:
                continue
                
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if 'metadata' in part_data and isinstance(part_data['metadata'], list):
                merged_data['metadata'].extend(part_data['metadata'])
                
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º processed_files
            if 'processed_files' in part_data and isinstance(part_data['processed_files'], list):
                merged_data['processed_files'].extend(part_data['processed_files'])
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        merged_data['processed_files'] = list(set(merged_data['processed_files']))
        
        return merged_data
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ JSON —á–∞—Å—Ç–µ–π: {e}")
        return None

def safe_read_json(file_path: str) -> dict:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ JSON —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º"""
    try:
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
        backup_path = str(Path(file_path).with_suffix('.bak'))
        shutil.copy2(file_path, backup_path)
        
        # –ß—Ç–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π BOM
        with open(file_path, 'rb') as f:
            content = f.read().decode('utf-8-sig')
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        content = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', content)
        content = content.replace('\\u0002', '')
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON —á–∞—Å—Ç–∏
        start = content.find('{')
        end = content.rfind('}') + 1
        
        if start == -1 or end == 0:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã JSON-—Å–∫–æ–±–∫–∏")
            
        return json.loads(content[start:end])
        
    except Exception as e:
        st.sidebar.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON:")
        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
        if os.path.exists(backup_path):
            try:
                with open(backup_path, 'rb') as f:
                    return json.loads(f.read().decode('utf-8-sig'))
            except Exception:
                pass
        raise

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')

    def preprocess(self, text: str) -> List[str]:
        text = self.regex.sub(' ', text.lower())
        return text.split()

class BM25SearchEngine:
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.bm25 = None
        self.chunks_info = []
        self.is_index_loaded = False
        self.cache_path = os.path.join("data", "bm25_index.json")
        self._load_index()
        self.llm_keywords = []  # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ—Ç LLM
        
    def _normalize_processed(self, processed_data: Any) -> List[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–µ processed –≤ –µ–¥–∏–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        if processed_data is None:
            return []
    
        if isinstance(processed_data, str):
            return [token for token in processed_data.split() if token]
        elif isinstance(processed_data, list):
            result = []
            for item in processed_data:
                if isinstance(item, str):
                    result.extend(item.split())
                elif isinstance(item, (int, float)):
                    result.append(str(item))
            return result
        else:
            return [str(processed_data)]

    def _load_index(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —á–∞—Å—Ç–µ–π
        merged_data = merge_json_parts(self.cache_path.replace('.json', '_part*.json'))
        
        if not merged_data:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–±–∏—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª
            if not os.path.exists(self.cache_path):
                st.sidebar.warning(f"–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.cache_path}")
                return False

            try:
                merged_data = safe_read_json(self.cache_path)
            except Exception as e:
                st.sidebar.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
                return False
        
        if not isinstance(merged_data, dict):
            st.sidebar.error("–ò–Ω–¥–µ–∫—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º")
            return False
            
        if 'metadata' not in merged_data:
            st.sidebar.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á 'metadata' –≤ –∏–Ω–¥–µ–∫—Å–µ")
            return False
        
        processed_texts = []
        valid_metadata = []
    
        for i, item in enumerate(merged_data.get('metadata', [])):
            if not isinstance(item, dict):
                st.sidebar.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —ç–ª–µ–º–µ–Ω—Ç {i} - –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º")
                continue
            
            original_text = item.get('original', '')
            processed = self._normalize_processed(item.get('processed', []))
        
            if not processed and original_text:
                processed = self.preprocessor.preprocess(original_text)
            
            if processed:
                processed_texts.append(processed)
                valid_metadata.append(item)
    
        if not processed_texts:
            st.sidebar.error("–ò–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return False
    
        self.bm25 = BM25Okapi(processed_texts)
        self.chunks_info = valid_metadata
        self.is_index_loaded = True
    
        st.sidebar.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å —Å {len(processed_texts)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö")
        return True

    def search(self, query: str, top_n: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        if not self.is_index_loaded:
            return []

        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Ç LLM –∫ –∑–∞–ø—Ä–æ—Å—É
            enhanced_query = f"{query} {' '.join(self.llm_keywords)}"
            tokens = self.preprocessor.preprocess(enhanced_query)
            
            if not tokens:
                return []

            if not hasattr(self.bm25, 'doc_freqs') or len(self.bm25.doc_freqs) == 0:
                return []

            if len(self.bm25.doc_len) == 0:
                return []

            scores = self.bm25.get_scores(tokens)
            if scores is None or len(scores) == 0:
                return []

            best_indices = np.argsort(scores)[-top_n:][::-1]

            return [
                {
                    'doc_id': self.chunks_info[idx].get('file_id', ''),
                    'doc_name': self.chunks_info[idx].get('doc_name', '–î–æ–∫—É–º–µ–Ω—Ç'),
                    'chunk_text': self.chunks_info[idx].get('original', '')[:2000],
                    'score': round(float(scores[idx]), 4)
                }
                for idx in best_indices
                if idx < len(self.chunks_info)
            ]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []            

class LLMClient:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        self.is_initialized = True

    def query(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        try:
            payload = {
                "model": "google/gemini-2.0-flash-lite-001",
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }

            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                
                raise Exception(f"API Error {response.status_code}: {error_detail}")

            return response.json()['choices'][0]['message']['content']
        except requests.exceptions.RequestException as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}")
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")

class DocumentAnalyzer:
    def __init__(self, api_url: str = None, api_key: str = None):
        self.preprocessor = TextPreprocessor()
        self.search_engine = BM25SearchEngine(self.preprocessor)
        self.current_docx = None
        self.llm_client = None
        self.llm_initialized = False
        self._initialize_llm(api_url, api_key)

    def _initialize_llm(self, api_url: str, api_key: str) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç LLM"""
        if not api_url or not api_key:
            self.llm_initialized = False
            return
            
        try:
            self.llm_client = LLMClient(api_url, api_key)
            self.llm_initialized = True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}")
            self.llm_initialized = False

    def _generate_keywords_from_text(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.llm_initialized or not text:
            return []
            
        try:
            messages = [
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏. –ò–∑–≤–ª–µ–∫–∞–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞."},
                {"role": "user", "content": f"{KEYWORDS_PROMPT}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{text[:10000]}"}
            ]
            
            response = self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords = []
            for line in response.split('\n'):
                if '‚Üí' in line:  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
                    parts = line.split('‚Üí')
                    for part in parts:
                        keywords.extend(re.findall(r'[\w\-]+', part.strip()))
                else:
                    keywords.extend(re.findall(r'[\w\-]+', line.strip()))
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            keywords = list(set(k.lower() for k in keywords if k.strip()))
            return keywords
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
            return []

    def analyze_document(self, prompt_type: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
        if not self.current_docx:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ DOCX —Ñ–∞–π–ª"
            
        docx_text = self.current_docx["content"]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if not self.search_engine.llm_keywords:
            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤..."):
                self.search_engine.llm_keywords = self._generate_keywords_from_text(docx_text)
                st.sidebar.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(self.search_engine.llm_keywords)}")
        
        query = self._generate_search_query(prompt_type, docx_text)
        chunks = self.search_engine.search(query)
        context = self._build_context(docx_text, chunks)
        
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": BUTTON_PROMPTS[prompt_type] + f"\n\n–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {st.session_state.doc_weight_slider:.1f}\n\n–ö–û–ù–¢–ï–ö–°–¢:\n" + context}
        ]

        st.sidebar.header("–ò—Ç–æ–≥–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM")
        st.sidebar.markdown("### –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:")
        st.sidebar.markdown(BUTTON_PROMPTS[prompt_type] + f"\n\n–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {st.session_state.doc_weight_slider:.1f}\n\n–ö–û–ù–¢–ï–ö–°–¢:\n" + context)
        
        return self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)

    def load_documents(self, uploaded_files) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DOCX —Ñ–∞–π–ª—ã"""
        if not uploaded_files:
            return

        try:
            for uploaded_file in uploaded_files:
                try:
                    if uploaded_file.size == 0:
                        st.warning(f"–§–∞–π–ª {uploaded_file.name} –ø—É—Å—Ç")
                        continue

                    file_bytes = io.BytesIO(uploaded_file.read())
                    doc = Document(file_bytes)
                    text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

                    if not text:
                        st.warning(f"–§–∞–π–ª {uploaded_file.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
                        continue

                    if len(text) > MAX_CONTEXT_LENGTH:
                        text = text[:MAX_CONTEXT_LENGTH]
                        st.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {uploaded_file.name} –æ–±—Ä–µ–∑–∞–Ω –¥–æ {MAX_CONTEXT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤")

                    self.current_docx = {
                        "name": uploaded_file.name,
                        "content": text
                    }

                    st.success(f"–î–æ–∫—É–º–µ–Ω—Ç {uploaded_file.name} –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {uploaded_file.name}: {str(e)}")
                    continue

        except Exception as e:
            st.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")

    def _generate_search_query(self, prompt_type: str, docx_text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è BM25"""
        base_queries = {
            "quality": "–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞",
            "strategy": "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–ø–æ—Ä–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ",
            "prediction": "–ø–æ–∑–∏—Ü–∏—è –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞"
        }
        
        return f"{base_queries[prompt_type]} {docx_text[:10000]}"

    def _build_context(self, docx_text: str, chunks: List[Dict]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM –∏–∑ DOCX –∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"""
        context_parts = [
            "=== –ó–ê–ì–†–£–ñ–ï–ù–ù–´–ô –î–û–ö–£–ú–ï–ù–¢ ===",
            docx_text,
            "\n=== –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô ==="
        ]
        
        for chunk in chunks:
            context_parts.append(f"\nüìÑ {chunk['doc_name']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk['score']:.2f}):")
            context_parts.append(chunk['chunk_text'][:3000])
        
        return "\n".join(context_parts)

def main():
    st.set_page_config(page_title="El Documente", layout="wide", initial_sidebar_state="collapsed")
    gif_path = "data/maracas-sombrero-hat.gif"
    st.image(gif_path, caption="Hola!", width=64)
    st.title("El Documente: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
    
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = DocumentAnalyzer(API_URL, API_KEY)
    
    analyzer = st.session_state.analyzer
    
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    col1, col2 = st.sidebar.columns([3, 1])
    with col1:
        weight = st.slider(
            "–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
            0.1, 1.0, 0.7, 0.1,
            key="doc_weight_slider",
            help="–†–µ–≥—É–ª–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"
        )

    with col2:
        st.metric("–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", f"{weight:.1f}")

    st.sidebar.write(f"–í—ã–±—Ä–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ: {weight}")
    
    if not analyzer.llm_initialized:
        st.sidebar.error("LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á –∏ URL")
    
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    uploaded_files = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ DOCX", 
        type=["docx"], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤..."):
            analyzer.load_documents(uploaded_files)
        st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(uploaded_files)}")

    CHAT_TEMPERATURE = 0.6
    CHAT_SYSTEM_PROMPT = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —é—Ä–∏—Å—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –ø—Ä–∞–≤–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ó–ê–ü–†–ï–©–ï–ù–û:  1. —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –≤—ã–¥—É–º–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –∏ —Å—É–¥–µ–±–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É. 2. —É–∫–∞–∑—ã–≤–∞—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ, —á—Ç–æ —Ç—ã –æ–∑–Ω–∞–∫–æ–º–∏–ª—Å—è —Å –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥–∏–∞–ª–æ–≥. –û—Ç–≤–µ—Ç—ã –∏–∑–ª–∞–≥–∞–π –≤ –¥–µ–ª–æ–≤–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏—Ö –º–Ω–µ–Ω–∏–π."""

    st.header("–ß–∞—Ç")

    user_input = st.text_area(
        "–û–±—Å—É–¥–∏—Ç—å —Å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º –ö–∞—Ä–ª–æ—Å–æ–º",
        max_chars=500,
        height=100
    )

    ask_button = st.button("–°–ø—Ä–æ—Å–∏—Ç—å", disabled=not (uploaded_files))# and user_input))

    if 'docx_added' not in st.session_state:
        st.session_state.docx_added = False

    if ask_button:
        # 1. –ì–æ—Ç–æ–≤–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        doc_summary = analyzer.current_docx["content"][:3000]  # –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        relevant_chunks = analyzer.search_engine.search(user_input)
    
        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Ä–æ–ª–µ–π
        messages = [
            {"role": "system", "content": CHAT_SYSTEM_PROMPT},
            {"role": "assistant", "content": f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (—Å–æ–∫—Ä–∞—â—ë–Ω–Ω–æ):\n{doc_summary}"},
            *[
                {"role": "assistant", "content": f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç ({chunk['doc_name']}):\n{chunk['chunk_text'][:800]}"}
                for chunk in relevant_chunks[:2]  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-2 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
            ],
            {"role": "user", "content": f"–î–∏–∞–ª–æ–≥:\n{'\n'.join(st.session_state.get('chat_history', []))[-2:]}"},
            {"role": "user", "content": user_input}
        ]
    
        # 3. –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        response = analyzer.llm_client.query(messages, temperature=0.7, max_tokens=1500)
        response_container = st.empty()
        response_container.markdown("### –û—Ç–≤–µ—Ç –æ—Ç –ö–∞—Ä–ª–æ—Å–∞")
        response_container.markdown(response)
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        st.session_state.setdefault('chat_history', []).extend([user_input, response])


    st.header("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    col1, col2, col3 = st.columns(3)

    buttons_disabled = not (uploaded_files and analyzer.llm_initialized)
    
    with col1:
        if st.button("–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞", disabled=buttons_disabled):
            with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞..."):
                result = analyzer.analyze_document("quality")
                st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏")
                st.markdown(result)
    
    with col2:
        if st.button("–î–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–ø–æ—Ä–∞", disabled=buttons_disabled):
            with st.spinner("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π..."):
                result = analyzer.analyze_document("strategy")
                st.markdown("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
                st.markdown(result)
    
    with col3:
        if st.button("–°–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã", disabled=buttons_disabled):
            with st.spinner("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏..."):
                result = analyzer.analyze_document("prediction")
                st.markdown("### –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞")
                st.markdown(result)

if __name__ == "__main__":
    main()
