# -*- coding: utf-8 -*-
import streamlit as st
import json
import os
import re
import pickle
#import rake_nltk
from docx import Document
from rank_bm25 import BM25Okapi
import numpy as np
from collections import defaultdict
import requests
from typing import List, Dict, Any
from rake_nltk import Rake
from pymorphy2 import MorphAnalyzer

try:
    from config import API_KEY, API_URL
except ImportError:
    st.error("–û—à–∏–±–∫–∞: –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª config.py —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ API_KEY –∏ API_URL")
    API_KEY = ""
    API_URL = "https://api.vsegpt.ru/v1/chat/completions"

#–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DATA_DIR = "data"
MAX_CONTEXT_LENGTH = 6000
MAX_ANSWER_LENGTH = 15000
TEMPERATURE = 0.2
os.makedirs(DATA_DIR, exist_ok=True)

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
SYSTEM_PROMPT = """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –∏–∑ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –¢—ã –º–æ–∂–µ—à—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å
–ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Ä–æ–≤. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã, –ø—É–Ω–∫—Ç—ã, –ø—Ä–∞–≤–∏–ª–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã,
–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

BUTTON_PROMPTS = {
    "quality": """–¢—ã —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –¢ –ü–ª—é—Å. –û—Ü–µ–Ω–∏ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ù–ê –û–°–ù–û–í–ï –≠–¢–ò–• –§–†–ê–ì–ú–ï–ù–¢–û–í:\n{context}\n\n. –ü—Ä–∞–≤–∏–ª–∞ –∞–Ω–∞–ª–∏–∑–∞: 
1. –ü–æ–ª–Ω–æ—Ç—É, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏, 
2. –æ—Ç–Ω–æ—Å–∏–º–æ—Å—Ç—å –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, 
3. –ø–æ–ª–Ω–æ—Ç—É —É–∫–∞–∑–∞–Ω–∏—è –≤ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω–∏–º—ã—Ö –Ω–æ—Ä–º –ø—Ä–∞–≤–∞. 
4. —Å—É–º–º–∞—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 1 –¥–æ 10 ["–Ø –±—ã –ø–æ—Å—Ç–∞–≤–∏–ª –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –¥–æ–∫—É–º–µ–Ω—Ç—É ... –∏–∑ 10]
5. –ø—Ä–∞–≤–∏–ª–∞ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ó–ê–ü–†–ï–©–ï–ù–û —Ü–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–∫–æ–Ω—ã, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö. –í—Å–µ –≤—ã–≤–æ–¥—ã –∏ —Ü–∏—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
6.[–í–ê–ñ–ù–û] –∫—Ä–∏—Ç–∏—á–Ω–æ –æ—Ü–µ–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º –¢ –ü–ª—é—Å –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤  \n{context}\n\n –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ), –ø—Ä–∏ –≤—ã—è–≤–ª–µ–Ω–∏–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–π - –ø—Ä–æ—Ü–∏—Ç–∏—Ä—É–π –Ω–∞—Ä—É—à–µ–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¢ –ü–ª—é—Å, –µ—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.""",

    "strategy": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ –∏ –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ù–∞–ø–∏—à–∏ –ø–æ—à–∞–≥–æ–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑, –∏—Å—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, —Å–º–µ–Ω—ã –∞–∫—Ü–µ–Ω—Ç–æ–≤ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –ü—Ä–µ–¥–ª–æ–∂–∏, –Ω–∞ —á–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è: –Ω–∞ —Å–±–æ—Ä–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –ø–æ–≤—ã—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤, –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –∏–ª–∏ —Å–º–µ–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.""",

    "prediction": """–¢—ã - —é—Ä–∏—Å—Ç-–ª–∏—Ç–∏–≥–∞—Ç–æ—Ä –≤ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏. –û—Ü–µ–Ω–∏ –ø—Ä–∞–≤–æ–≤—É—é —Å–∏—Ç—É–∞—Ü–∏—é —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª–∞–±—ã—Ö –∏ —Å–∏–ª—å–Ω—ã—Ö –º–µ—Å—Ç –≤ –ø–æ–∑–∏—Ü–∏–∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏ —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –¥–æ–∫–∞–∑—ã–≤–∞—Ç—å‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –º–æ–∂–µ—Ç —É—Å–∏–ª–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é –≤ —á–∞—Å—Ç–∏‚Ä¶], [–æ–ø–ø–æ–Ω–µ–Ω—Ç –ø–æ–ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–æ–≤–µ—Ä–≥–∞—Ç—å‚Ä¶], —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏, –∫–∞–∫–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å—Ö–æ–¥ –¥–µ–ª–∞ –ø—Ä–∏ –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ–º –¥–ª—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Å–∏—Ç—É–∞—Ü–∏–∏."""
}

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')

    def preprocess(self, text: str) -> List[str]:
        text = self.regex.sub(' ', text.lower())
        return text.split()

class BM25SearchEngine:
    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self.bm25 = None
        self.chunks_info = []
        self.doc_index = defaultdict(list)
        self.is_index_loaded = False

    def build_index(self, documents: List[Dict]) -> None:
        corpus = []
        self.chunks_info = []

        for doc_idx, doc in enumerate(documents):
            self.chunks_info.append({
                'doc_id': doc.get("name", f"doc_{doc_idx}"),
                'doc_name': doc.get("name", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"),
                'chunk_text': doc.get("content", "")
            })
            corpus.append(doc.get("content", ""))

        tokenized_corpus = [self.preprocessor.preprocess(doc) for doc in corpus]
        self.bm25 = BM25Okapi(tokenized_corpus)
        self.is_index_loaded = True

    def load_from_cache(self, cache_path: str) -> bool:
        try:
            if not os.path.exists(cache_path):
                return False

            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
                self.bm25, self.chunks_info, self.doc_index = data
                self.is_index_loaded = True
                return True
        except:
            return False

    def save_to_cache(self, cache_path: str) -> None:
        with open(cache_path, 'wb') as f:
            pickle.dump((self.bm25, self.chunks_info, self.doc_index), f)

    def search(self, query: str, top_n: int = 5) -> List[Dict]:
        if not self.is_index_loaded:
            return []

        tokens = self.preprocessor.preprocess(query)
        if not tokens:
            return []

        scores = self.bm25.get_scores(tokens)
        best_indices = np.argsort(scores)[-top_n:][::-1]

        results = []
        for idx in best_indices:
            result = {**self.chunks_info[idx], 'score': float(scores[idx])}
            results.append(result)

        return results
class LLMClient:
    def __init__(self, api_url: str, api_key: str):
        self.api_url = api_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        self.is_initialized = True

    def query(self, messages: List[Dict], temperature: float, max_tokens: int) -> str:
        try:
            payload = {
                "model": "google/gemini-2.0-flash-lite-001", #"qwen/qwq-32b",
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": False
            }

            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                
                raise Exception(f"API Error {response.status_code}: {error_detail}")

            return response.json()['choices'][0]['message']['content']
        except requests.exceptions.RequestException as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}")
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")


class DocumentAnalyzer:
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.search_engine = BM25SearchEngine(self.preprocessor)
        self.llm_client = None
        self.documents = []
        self.llm_initialized = False

    def initialize_llm(self, api_url: str, api_key: str):
        if not api_key:
            raise ValueError("API Key –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        
        self.llm_client = LLMClient(api_url, api_key)
        self.llm_initialized = True
        return True

    def load_documents(self, uploaded_files):
        self.documents = []
        for file in uploaded_files:
            try:
                doc = Document(file)
                text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
                
                if len(text) > MAX_CONTEXT_LENGTH:
                    truncate_ratio = MAX_CONTEXT_LENGTH / len(text)
                    cutoff = int(len(text) * truncate_ratio)
                    text = text[:cutoff]
                    st.warning(f"–î–æ–∫—É–º–µ–Ω—Ç {file.name} –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω –¥–æ {cutoff} —Å–∏–º–≤–æ–ª–æ–≤")
                
                self.documents.append({
                    "name": file.name,
                    "content": text
                })
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file.name}: {str(e)}")
        
        if self.documents:
            self.search_engine.build_index(self.documents)
            with open(os.path.join(DATA_DIR, "documents.json"), "w") as f:
                json.dump(self.documents, f)
            self.search_engine.save_to_cache(os.path.join(DATA_DIR, "bm25_index.pkl"))

    def analyze_document(self, prompt_type: str) -> str:
        if not self.documents:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        full_text = " ".join([doc["content"] for doc in self.documents])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –∑–∞–ø—Ä–æ—Å–∞
        SEARCH_WEIGHTS = {
            "base_query": 0.1,    # –í–µ—Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            "doc_content": 1.0    # –í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        base_queries = {
            "quality": "–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞",
            "strategy": "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–ø–æ—Ä–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ",
            "prediction": "–ø–æ–∑–∏—Ü–∏—è –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞"
        }
        
        # –£—Å–∏–ª–∏–≤–∞–µ–º –≤–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –ø–æ–≤—Ç–æ—Ä–∞–º–∏
        boosted_query = (
            f"{base_queries[prompt_type]} " * int(SEARCH_WEIGHTS["base_query"] * 10) +
            f"{full_text[:1000]} " * int(SEARCH_WEIGHTS["doc_content"] * 10)
        )
        
        # –ü–æ–∏—Å–∫ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
        chunks = self.search_engine.search(boosted_query)
        combined_query = f"{BUTTON_PROMPTS[prompt_type]} {full_text[:1000]}"
        context = self._build_context(chunks)

        with st.expander("üîç –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞", expanded=False):
            st.write("### –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç:")
            st.code(SYSTEM_PROMPT, language="text")
            
            st.write("### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç:")
            st.code(BUTTON_PROMPTS[prompt_type], language="text")
            
            st.write("### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ BM25:")
            st.json({
                "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å": combined_query,
                "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã": [
                    {"–î–æ–∫—É–º–µ–Ω—Ç": chunk["doc_name"], "–¢–µ–∫—Å—Ç": chunk["chunk_text"][:200]} 
                    for chunk in chunks
                ]
            })
            
            st.write("### –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM:")
            st.text_area("–ö–æ–Ω—Ç–µ–∫—Å—Ç", value=context, height=300, label_visibility="collapsed")
        # ===== –ö–û–ù–ï–¶ –í–´–í–û–î–ê –ö–û–ù–¢–ï–ö–°–¢–ê =====


        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": BUTTON_PROMPTS[prompt_type] + "\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n" + context}
        ]
        
        return self.llm_client.query(messages, TEMPERATURE, MAX_ANSWER_LENGTH)
        #except Exception as e:
            #return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}"

    def _build_context(self, chunks: List[Dict]) -> str:
        context = ["–ù–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (–ø–æ–∏—Å–∫ BM25):"]
        for chunk in chunks:
            context.append(f"\nüìÑ {chunk['doc_name']} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk['score']:.2f}):")
            context.append(chunk['chunk_text'][:1000])
        return "\n".join(context)

def main():
    st.set_page_config(page_title="El Documente", layout="wide")
    st.title("El Documente: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    col1, col2 = st.sidebar.columns([3, 1])
    with col1:
        weight = st.slider(
            "–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
            0.1, 2.0, 0.7, 0.1,
            key="doc_weight_slider",
            help="–†–µ–≥—É–ª–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"
        )

    with col2:
        st.metric("–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ", f"{weight:.1f}")

    # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ)
    st.sidebar.write(f"–í—ã–±—Ä–∞–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ: {weight}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = DocumentAnalyzer()
    
    analyzer = st.session_state.analyzer
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
    if not hasattr(analyzer, 'llm_initialized') or not analyzer.llm_initialized:
        try:
            if analyzer.initialize_llm(API_URL, API_KEY):
                st.session_state.llm_initialized = True
                st.sidebar.success("LLM —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            else:
                st.sidebar.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LLM")
        except Exception as e:
            st.sidebar.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        weights = st.sidebar.slider(
            "–í–µ—Å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –ø–æ–∏—Å–∫–µ",
            0.1, 2.0, 0.7, 0.1
        )
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    uploaded_files = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ DOCX", 
        type=["docx"], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤..."):
            analyzer.load_documents(uploaded_files)
        st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(uploaded_files)}")
    
    # –ö–Ω–æ–ø–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
    st.header("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    col1, col2, col3 = st.columns(3)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    buttons_disabled = not (uploaded_files and hasattr(analyzer, 'llm_initialized') and analyzer.llm_initialized)
    
    with col1:
        if st.button("–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞", disabled=buttons_disabled):
            with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞..."):
                result = analyzer.analyze_document("quality")
                st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏")
                st.markdown(result)  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–Ω–¥–µ—Ä–∏—Ç Markdown
    
    with col2:
        if st.button("–î–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–ø–æ—Ä–∞", disabled=buttons_disabled):
            with st.spinner("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π..."):
                result = analyzer.analyze_document("strategy")
                st.markdown("### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
                st.markdown(result)
    
    with col3:
        if st.button("–°–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –≤—Ç–æ—Ä–æ–π —Å—Ç–æ—Ä–æ–Ω—ã", disabled=buttons_disabled):
            with st.spinner("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏..."):
                result = analyzer.analyze_document("prediction")
                st.markdown("### –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–∑–∏—Ü–∏–∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞")
                st.markdown(result)

if __name__ == "__main__":
    main()
