import streamlit as st

st.title("üéà My new app")
st.write(
    "Let's start building! For help and inspiration, head over to [docs.streamlit.io](https://docs.streamlit.io/)."
)


import streamlit as st
import rake-nltk
import json
import os
from pathlib import Path
import uuid
import re
from PyPDF2 import PdfReader
import docx
import requests
import numpy as np
from rank_bm25 import BM25Okapi
from config import API_KEY, API_URL

# Constants
LLM = "anthropic/claude-3-haiku"
CHUNK_SIZE = 12000
CONTEXT_SUM = 4000
MAX_ANSWER_LENGTH = 4000
TEMPERATURE = 0.4

# Base prompts
DEFAULT_PROMPT = """–ò–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ/–ø—É–±–ª–∏—Ü–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ —Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π:
doc_name: [–ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
doc_date: [–¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ì–ì–ì–ì-–ú–ú-–î–î –∏–ª–∏ "–ù–µ —É–∫–∞–∑–∞–Ω–∞"]
doc_type: [—Ç–∏–ø: –∑–∞–∫–æ–Ω, —É–∫–∞–∑, –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ, —Å—É–¥–µ–±–Ω—ã–π –∞–∫—Ç, —Å—Ç–∞—Ç—å—è –∏–ª–∏ –∏–Ω–æ–µ]
chunk_summary: [—Ç—Ä–∏ —Ç–µ–∑–∏—Å–∞ –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏]
qa_pairs: [3 –ø–∞—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–≤–æ–ø—Ä–æ—Å:: –æ—Ç–≤–µ—Ç" (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å - –¥–≤–∞ –¥–≤–æ–µ—Ç–æ—á–∏—è)]
chunk_keywords: [3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞, 3 –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑—ã]"""

class DocumentProcessor:
    def __init__(self):
        self.knowledge_base = {}
        
    def read_file(self, file):
        try:
            if file.name.endswith('.pdf'):
                reader = PdfReader(file)
                return "\n".join([page.extract_text() for page in reader.pages])
            elif file.name.endswith('.docx'):
                doc = docx.Document(file)
                return "\n".join([para.text for para in doc.paragraphs])
            else:
                return file.getvalue().decode('utf-8')
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file.name}: {str(e)}")
            return ""

    def process_document(self, text, prompt_template):
        chunks = self.split_text(text)
        doc_id = str(uuid.uuid4())
        doc_data = {
            "doc_id": doc_id,
            "chunks": [],
            "doc_summary": "",
            "doc_keywords": [],
        }

        for i, chunk in enumerate(chunks):
            response = self.send_llm_request(
                prompt_template + f"\n\n–¢–µ–∫—Å—Ç: {chunk[:5000]}..."
            )
            chunk_data = self.parse_llm_response(response, i == 0)
            chunk_data['chunk_text'] = chunk
            
            if i == 0:
                doc_data.update({
                    "doc_name": chunk_data.get('doc_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                    "doc_date": chunk_data.get('doc_date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞'),
                    "doc_type": chunk_data.get('doc_type', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')
                })

            doc_data['chunks'].append(chunk_data)

        return doc_data

    def split_text(self, text):
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            if current_length + len(sentence) > CHUNK_SIZE and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(sentence)
            current_length += len(sentence)

        if current_chunk:
            chunks.append(' '.join(current_chunk))
        return chunks

    def send_llm_request(self, prompt):
        headers = {
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
        data = {
            "model": LLM,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 2000
        }
        
        try:
            response = requests.post(API_URL, json=data, headers=headers)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ API: {str(e)}")
            return ""

    def parse_llm_response(self, response, is_first_chunk=False):
        parsed = {
            'chunk_summary': '',
            'chunk_text': '',
            'qa_pairs': [],
            'chunk_keywords': []
        }
        
        if is_first_chunk:
            parsed.update({
                'doc_name': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                'doc_date': '–ù–µ —É–∫–∞–∑–∞–Ω–∞',
                'doc_type': '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω'
            })

        lines = response.split('\n')
        current_section = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            if is_first_chunk:
                if line.startswith('doc_name:'):
                    parsed['doc_name'] = line.split(':', 1)[1].strip()
                elif line.startswith('doc_date:'):
                    parsed['doc_date'] = line.split(':', 1)[1].strip()
                elif line.startswith('doc_type:'):
                    parsed['doc_type'] = line.split(':', 1)[1].strip()

            if line.startswith('chunk_summary:'):
                current_section = 'summary'
                parsed['chunk_summary'] = line.split(':', 1)[1].strip()
            elif line.startswith('qa_pairs:'):
                current_section = 'qa'
            elif line.startswith('chunk_keywords:'):
                current_section = 'keywords'
            elif '::' in line and current_section == 'qa':
                q, a = line.split('::', 1)
                parsed['qa_pairs'].append({'question': q.strip(), 'answer': a.strip()})
            elif current_section == 'keywords':
                parsed['chunk_keywords'].append(line.strip())

        return parsed

class SearchEngine:
    def __init__(self):
        self.preprocessor = TextPreprocessor()
        self.bm25 = None
        self.chunks_info = []
        
    def build_index(self, knowledge_base):
        corpus = []
        self.chunks_info = []
        
        for doc in knowledge_base.get('documents', []):
            for chunk in doc.get('chunks', []):
                text_parts = [
                    str(chunk.get('chunk_summary', '')),
                    ' '.join(map(str, chunk.get('chunk_keywords', []))),
                    str(chunk.get('chunk_text', ''))
                ]
                text_for_index = ' '.join(text_parts)
                corpus.append(text_for_index)
                
                self.chunks_info.append({
                    'doc_name': doc.get('doc_name', ''),
                    'chunk_summary': chunk.get('chunk_summary', ''),
                    'chunk_text': chunk.get('chunk_text', ''),
                    'chunk_keywords': chunk.get('chunk_keywords', [])
                })
                
        tokenized_corpus = [self.preprocessor.preprocess(doc) for doc in corpus]
        self.bm25 = BM25Okapi(tokenized_corpus)

    def search(self, query, top_n=5):
        if not self.bm25:
            return []
            
        tokens = self.preprocessor.preprocess(query)
        scores = self.bm25.get_scores(tokens)
        best_indices = np.argsort(scores)[-top_n:][::-1]
        
        results = []
        for idx in best_indices:
            if scores[idx] > 0.1:
                result = {**self.chunks_info[idx], 'score': float(scores[idx])}
                results.append(result)
        return results

class TextPreprocessor:
    def __init__(self):
        self.regex = re.compile(r'[^\w\s]')
        
    def preprocess(self, text):
        text = self.regex.sub(' ', text.lower())
        return text.split()

def main():
    st.set_page_config(page_title="–î–æ–∫—É–º–µ–Ω—Ç-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç", layout="wide")
    st.title("–î–æ–∫—É–º–µ–Ω—Ç-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")

    if 'processor' not in st.session_state:
        st.session_state.processor = DocumentProcessor()
        st.session_state.search_engine = SearchEngine()
        
    # –°–∞–π–¥–±–∞—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
    with st.sidebar:
        st.header("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π")
        kb_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π", type=['json'])
        if kb_file:
            st.session_state.knowledge_base = json.load(kb_file)
            st.session_state.search_engine.build_index(st.session_state.knowledge_base)
            st.success("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        save_path = st.text_input("–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π", "knowledge_base.json")
        if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞–∑—É"):
            try:
                with open(save_path, 'w', encoding='utf-8') as f:
                    json.dump(st.session_state.knowledge_base, f, ensure_ascii=False, indent=2)
                st.success("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")

    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    tab1, tab2 = st.tabs(["–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"])
    
    with tab1:
        st.header("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        uploaded_files = st.file_uploader(
            "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
            type=['txt', 'pdf', 'docx'],
            accept_multiple_files=True
        )
        
        with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–º–ø—Ç–∞"):
            prompt = st.text_area("–ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏", DEFAULT_PROMPT, height=300)
            
        if st.button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã") and uploaded_files:
            progress_bar = st.progress(0)
            for i, file in enumerate(uploaded_files):
                text = st.session_state.processor.read_file(file)
                if text:
                    doc_data = st.session_state.processor.process_document(text, prompt)
                    if 'documents' not in st.session_state.knowledge_base:
                        st.session_state.knowledge_base['documents'] = []
                    st.session_state.knowledge_base['documents'].append(doc_data)
                progress_bar.progress((i + 1) / len(uploaded_files))
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
    with tab2:
        st.header("–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å")
        
        with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM"):
            llm_prompt = st.text_area(
                "–ü—Ä–æ–º–ø—Ç –¥–ª—è LLM",
                "–¢—ã - AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç—ã. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç–µ–π –∏ –ø—É–Ω–∫—Ç–æ–≤.",
                height=100
            )
            
        if st.button("–ò—Å–∫–∞—Ç—å") and query:
            results = st.session_state.search_engine.search(query)
            if results:
                context = build_llm_context(query, results)
                response = st.session_state.processor.send_llm_request(
                    f"{llm_prompt}\n\n{context}"
                )
                st.write("–û—Ç–≤–µ—Ç:")
                st.write(response)
            else:
                st.warning("–ü–æ –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

def build_llm_context(query, chunks):
    context_parts = [
        f"–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}",
        "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:"
    ]
    
    for chunk in chunks:
        context_parts.extend([
            f"\n–î–æ–∫—É–º–µ–Ω—Ç: {chunk.get('doc_name', '')}",
            f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(chunk.get('chunk_keywords', []))}",
            f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {chunk.get('chunk_text', '')[:1000]}"
        ])
        
    return '\n'.join(context_parts)[:CONTEXT_SUM]

if __name__ == "__main__":
    main()
